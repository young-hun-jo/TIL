{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "national-romance",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#집계-연산\" data-toc-modified-id=\"집계-연산-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>집계 연산</a></span><ul class=\"toc-item\"><li><span><a href=\"#집계-함수\" data-toc-modified-id=\"집계-함수-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>집계 함수</a></span><ul class=\"toc-item\"><li><span><a href=\"#count\" data-toc-modified-id=\"count-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>count</a></span></li><li><span><a href=\"#countDistinct\" data-toc-modified-id=\"countDistinct-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>countDistinct</a></span></li><li><span><a href=\"#approx_count_distinct\" data-toc-modified-id=\"approx_count_distinct-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>approx_count_distinct</a></span></li><li><span><a href=\"#first,-last\" data-toc-modified-id=\"first,-last-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>first, last</a></span></li><li><span><a href=\"#min,-max\" data-toc-modified-id=\"min,-max-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>min, max</a></span></li><li><span><a href=\"#sum\" data-toc-modified-id=\"sum-1.1.6\"><span class=\"toc-item-num\">1.1.6&nbsp;&nbsp;</span>sum</a></span></li><li><span><a href=\"#sumDistinct\" data-toc-modified-id=\"sumDistinct-1.1.7\"><span class=\"toc-item-num\">1.1.7&nbsp;&nbsp;</span>sumDistinct</a></span></li><li><span><a href=\"#avg,-mean\" data-toc-modified-id=\"avg,-mean-1.1.8\"><span class=\"toc-item-num\">1.1.8&nbsp;&nbsp;</span>avg, mean</a></span></li><li><span><a href=\"#분산과-표준편차\" data-toc-modified-id=\"분산과-표준편차-1.1.9\"><span class=\"toc-item-num\">1.1.9&nbsp;&nbsp;</span>분산과 표준편차</a></span></li><li><span><a href=\"#비대칭도(skewness),-첨도(kurtosis)\" data-toc-modified-id=\"비대칭도(skewness),-첨도(kurtosis)-1.1.10\"><span class=\"toc-item-num\">1.1.10&nbsp;&nbsp;</span>비대칭도(skewness), 첨도(kurtosis)</a></span></li><li><span><a href=\"#공분산과-상관관계\" data-toc-modified-id=\"공분산과-상관관계-1.1.11\"><span class=\"toc-item-num\">1.1.11&nbsp;&nbsp;</span>공분산과 상관관계</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-cutting",
   "metadata": {},
   "source": [
    "# 집계 연산\n",
    "- 집계 시 인지해야 할 중요한 점은\n",
    "    - 집계한다는 것은 빅데이터에서 정확한 답을 얻기 위함임. 그런데 어느정도 즉, 수용가능한 정도의 정확도에 맞춰 근사치를 계산하는 것이 연산, 네트워크, 저장소 등의 비용을 고려하는 것임\n",
    "    - 따라서 **근사치 계산용 함수**를 사용해 스파크 잡의 실행과 속도를 개선할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "statewide-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "          .option('header', 'true')\\\n",
    "          .option('inferSchema', 'true')\\\n",
    "          .load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/retail-data/all/*.csv')\\\n",
    "          .coalesce(5)  # 컬레스를 사용해 리파티셔닝(파티션 감소)\n",
    "\n",
    "# 해당 데이터프레임을 메모리에 캐싱\n",
    "df.cache()\n",
    "# 트랜스포메이션 결과(액션) 관찰하기 위해 임시 테이블로 생성\n",
    "df.createOrReplaceTempView('dfTable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-static",
   "metadata": {},
   "source": [
    "- ``count()``메소드는 즉시 수행하는 액션 메소드임! \n",
    "    - 데이터프레임의 행 개수를 집계하는 데 사용되기도 하지만 메모리에 데이터프레임 캐싱작업을 수행하는 용도로도 사용됨!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "disturbed-zoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541909\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "count = df.count()\n",
    "print(count)\n",
    "print(count == 541909)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-manhattan",
   "metadata": {},
   "source": [
    "## 집계 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-insider",
   "metadata": {},
   "source": [
    "### count\n",
    "- 해당 예제에서는 액션이 아닌 **트랜스포메이션(lazy execution)** 으로 동작함\n",
    "- 두 가지 방식으로 사용 가능\n",
    "    * ``count(\"column_name\")``\n",
    "        - 특정 칼럼명을 지정하면 ``null``값 제외하고 집계\n",
    "    * ``count(*)``\n",
    "        - 단, SQL과 마착나지로 ``*`` 사용해서 카운트 시, 만약 칼럼들에 ``null``값이 포함되어 있다면 ``null``이 있는 것도 모두 집계\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interim-cutting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "capital-breast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| count|\n",
      "+------+\n",
      "|541909|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count \n",
    "\n",
    "df.select(count('StockCode').alias('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "alert-electron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(count('*')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-questionnaire",
   "metadata": {},
   "source": [
    "### countDistinct\n",
    "\n",
    "- unique한 row 개수를 출력하는데, 이 메소드는 주로 개별 컬럼에다가 적용하는것이 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "constant-insurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Customer_cnt|\n",
      "+------------+\n",
      "|        4372|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.select(countDistinct('CustomerID').alias('Customer_cnt')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-harbor",
   "metadata": {},
   "source": [
    "### approx_count_distinct\n",
    "- 대규모 데이터셋을 다루다 보면 정확한 unique한 개수는 필요없고 대략적인 근사치만 필요할 때가 있음\n",
    "- 추가 파라미터인 최대 추정 오류율(maximum estimation error)이 존재하는데, 0.1이면 오류율 10%을 의미\n",
    "    - 단, 39%의 오류율까지만 허용함. 그 이상 입력 시 에러 발생\n",
    "- ``countDistinct``보다 훨씬 속도가 빠르며, 데이터셋이 매우 커짐에 따라 해당 메서드의 유용성이 증가(단, 대략적인 근사치가 필요할 때만!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "professional-tennessee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|approx_Customer_cnt|\n",
      "+-------------------+\n",
      "|               4336|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df.select(approx_count_distinct('CustomerID', 0.1).alias('approx_Customer_cnt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "assured-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|approx_Customer_cnt|\n",
      "+-------------------+\n",
      "|               5955|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df.select(approx_count_distinct('CustomerID', 0.39).alias('approx_Customer_cnt')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-anthony",
   "metadata": {},
   "source": [
    "### first, last\n",
    "- 데이터프레임 특정 칼럼의 첫 번째 값 또는 마지막 값을 반환\n",
    "- 이 함수들은 Row-based으로 동작함\n",
    "    - Row-based/Column-based 설명 <a href='https://bi-insider.com/business-intelligence/column-and-row-based-database-storage/'>링크</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "naked-preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----------------------------+\n",
      "|first(Description)                |last(Description)            |\n",
      "+----------------------------------+-----------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|BAKING SET 9 PIECE RETROSPOT |\n",
      "+----------------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "df.select(first('Description'), last('Description')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-entrepreneur",
   "metadata": {},
   "source": [
    "### min, max\n",
    "- 데이터프레임 특정 칼럼의 최솟값, 최대값을 반환(문자열도 오름,내림차순으로 자동 정렬 후 추출됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "classical-argentina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------+--------------+\n",
      "|max(Country)|min(Country)|max(UnitPrice)|min(UnitPrice)|\n",
      "+------------+------------+--------------+--------------+\n",
      "| Unspecified|   Australia|       38970.0|     -11062.06|\n",
      "+------------+------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min \n",
    "\n",
    "df.select(max('Country'), min('Country'), max('UnitPrice'), min('UnitPrice')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-dynamics",
   "metadata": {},
   "source": [
    "### sum\n",
    "- 모든 값을 합산\n",
    "- 문자열을 sum 하면 null로 반환 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "civic-numbers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|  sum(UnitPrice)|\n",
      "+----------------+\n",
      "|2498803.97400038|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df.select(sum('UnitPrice')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "spoken-platform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|sum(Country)|\n",
      "+------------+\n",
      "|        null|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 문자열을 sum 하면 null로 반환 됨\n",
    "df.select(sum('Country')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-officer",
   "metadata": {},
   "source": [
    "### sumDistinct\n",
    "- unique한 값들만 합산 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "induced-conservative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|sum(DISTINCT UnitPrice)|\n",
      "+-----------------------+\n",
      "|      611388.3910000001|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "\n",
    "df.select(sumDistinct('UnitPrice')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-copper",
   "metadata": {},
   "source": [
    "### avg, mean\n",
    "- ``sum / count``로 평균값을 직접 구할 수 있지만, avg, mean으로 대체 가능\n",
    "    - ``avg``는 ``pyspark.sql.functions`` 패키지에 있지만,\n",
    "    - ``mean``은 ``expr`` 메소드를 사용한 후 쿼리문 형태로 사용 가능함\n",
    "- 참고로 대부분의 집계 함수는 unique한 값들만 사용해 집계를 수행하는 방식을 지원함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "further-finnish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|    avg_quantity|   mean_quantity|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "df.select(count('Quantity').alias('total_transactions'),\n",
    "         sum('Quantity').alias('total_purchases'),\n",
    "         avg('Quantity').alias('avg_quantity'),\n",
    "         expr('mean(Quantity)').alias('mean_quantity')) \\\n",
    "  .selectExpr('total_purchases / total_transactions',\n",
    "             'avg_quantity',\n",
    "             'mean_quantity').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-laser",
   "metadata": {},
   "source": [
    "### 분산과 표준편차\n",
    "- 스파크는 표본분산(표준편차)와 모분산(표준편차)를 개별로 제공\n",
    "    - 위 2개 계산 방식은 다르기 떄문에 메서드도 다름\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "upper-council",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+---------------------+--------------------+\n",
      "|var_samp(Quantity)| var_pop(Quantity)|stddev_samp(Quantity)|stddev_pop(Quantity)|\n",
      "+------------------+------------------+---------------------+--------------------+\n",
      "|47559.391409298754|47559.303646609056|   218.08115785023418|  218.08095663447796|\n",
      "+------------------+------------------+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_samp, stddev_samp # sample = 표본\n",
    "from pyspark.sql.functions import var_pop, stddev_pop   # pop=population = 모집단\n",
    "\n",
    "df.select(var_samp('Quantity'), var_pop('Quantity'), stddev_samp('Quantity'), stddev_pop('Quantity')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-nation",
   "metadata": {},
   "source": [
    "### 비대칭도(skewness), 첨도(kurtosis)\n",
    "- 확률변수, 확률분포를 기반으로 데이터를 모델링할 때 해당 척도들을 알아보는 것이 중요할 때 있음\n",
    "- 값을 모두 정규화시킨 후, 왜도, 첨도를 계산했을 때,\n",
    "    - 왜도\n",
    "        - 양수: 왼쪽으로 기울어짐(오른쪽으로 꼬리가 긴)\n",
    "        - 음수: 오른쪽으로 기울어짐(왼쪽으로 꼬리가 긴)\n",
    "        - 0: 정규분포 모양과 유사\n",
    "    - 첨도\n",
    "        - 양수: 뾰족함(즉, 표준편차값이 작을 때 모양)\n",
    "        - 음수: 퍼져있음(즉, 표준편차값이 클 때 모양)\n",
    "        - 0: 정규분포 모양과 유사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "textile-rhythm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|skewness(UnitPrice)|kurtosis(UnitPrice)|\n",
      "+-------------------+-------------------+\n",
      "| 186.50645547025985| 59005.174662675585|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "df.select(skewness(\"UnitPrice\"), kurtosis(\"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-elevation",
   "metadata": {},
   "source": [
    "### 공분산과 상관관계\n",
    "- 공분산은 데이터 입력의 범위를 고려하지 않음. 그래서 ``공분산 --(normalize)--> 상관계수``\n",
    "- 공분산도 모공분산, 표본공분산으로 메소드가 나뉘므로 사용상황에 따라 잘 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "included-dublin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|corr(Quantity, UnitPrice)|covar_pop(Quantity, UnitPrice)|covar_samp(Quantity, UnitPrice)|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|     -0.00123492454487...|           -26.058713170967906|            -26.058761257936858|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\"), covar_pop(\"Quantity\", \"UnitPrice\"), covar_samp(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-commercial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
