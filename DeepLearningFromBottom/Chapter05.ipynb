{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#오차역전파-적용한-신경망-구현\" data-toc-modified-id=\"오차역전파-적용한-신경망-구현-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>오차역전파 적용한 신경망 구현</a></span></li><li><span><a href=\"#수치미분-활용해-오차역전파법으로-구한-기울기-값-검증\" data-toc-modified-id=\"수치미분-활용해-오차역전파법으로-구한-기울기-값-검증-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>수치미분 활용해 오차역전파법으로 구한 기울기 값 검증</a></span></li><li><span><a href=\"#오차역전파를-사용한-MNIST-데이터-학습-구현\" data-toc-modified-id=\"오차역전파를-사용한-MNIST-데이터-학습-구현-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>오차역전파를 사용한 MNIST 데이터 학습 구현</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "순전파 수행 후 지불해야 할 최종 금액: 220.00000000000003\n",
      "\n",
      "역전파 수행 후 각 변수의 변화량 값\n",
      "사과 박스 가격: 2.2\n",
      "사과 박스 개수: 110.00000000000001\n",
      "소비자세: 200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x * y\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        # 순전파 시 입력을 서로 바꾸어서 곱해줌!\n",
    "        dx = d_out * self.y\n",
    "        dy = d_out * self.x\n",
    "        return dx, dy\n",
    "    \n",
    "apple_box = 100\n",
    "apple_box_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 곱셈계층\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파 수행\n",
    "apple_box_price = mul_apple_layer.forward(apple_box, apple_box_num)\n",
    "price = mul_tax_layer.forward(apple_box_price, tax)\n",
    "print('순전파 수행 후 지불해야 할 최종 금액:', price)\n",
    "print()\n",
    "\n",
    "# 역전파 수행(순전파와 반대 순서로 호출)\n",
    "d_price = 1\n",
    "d_apple_box_price, d_tax = mul_tax_layer.backward(d_price)\n",
    "d_apple_box, d_apple_box_num = mul_apple_layer.backward(d_apple_box_price)\n",
    "print('역전파 수행 후 각 변수의 변화량 값')\n",
    "print('사과 박스 가격:', d_apple_box)\n",
    "print('사과 박스 개수:', d_apple_box_num)\n",
    "print('소비자세:', d_tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        dx = d_out * 1\n",
    "        dy = d_out * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 블로깅 완료\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Relu 활성함수의 역전파 계층 만들기\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x: np.array):\n",
    "        self.mask = (x <= 0) # 배열 원소값이 0이하인 값들에 대한 Boolean 인덱싱\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        d_out[self.mask] = 0 # 순전파 시 입력값이 0이하인 데이터에 대한 국소적인 미분값은 0으로 전달하도록 변환\n",
    "        dx = d_out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid 활성함수의 역전파 계층 만들기 -> 결국 최종 변화량은 1(d_out) * y(1-y)가 됨!\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x: np.array):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        dx = d_out * self.out(1 - self.out)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "np.expand_dims(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine(행렬 곱) 계층 구현 -> 4차원인 경우 고려한 코드랑 차이가 있음!\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # out = x * W + b\n",
    "        self.x = x\n",
    "        out = np.matmul(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        dx = np.matmul(d_out, self.W.T)\n",
    "        self.dW = np.matmul(self.x.T, d_out)\n",
    "        self.db = np.sum(d_out, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sofmtax-with-Loss 계층 구현\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None   # prediction\n",
    "        self.t = None   # label\n",
    "        \n",
    "    def softmax(x):\n",
    "        max_x = np.max(x)\n",
    "        exp_x = np.exp(x - max_x)\n",
    "        exp_x_sum = np.sum(exp_x)\n",
    "        \n",
    "        return exp_x / exp_x_sum\n",
    "    \n",
    "    def cross_entropy_error(y, t):\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "\n",
    "        # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1)\n",
    "\n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(x)\n",
    "        self.loss = self.cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 오차역전파 적용한 신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#===============\n",
    "# 필요한 함수들 구현\n",
    "#===============\n",
    "def sigmoid(x: np.array):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x: np.array):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x: np.array):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y: np.array, t: np.array):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, t.size)\n",
    "    # 학습 데이터의 레이블이 One-hot 형태라면 Label 형태로 바꾸어주기\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "# 수치 미분 함수\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h)\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fx1 = f(x)\n",
    "        # f(x-h)\n",
    "        x[idx] = float(tmp_val) - h\n",
    "        fx2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fx1 - fx2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad\n",
    "\n",
    "#===============\n",
    "# 필요한 계층들 구현 \n",
    "#===============\n",
    "# 1.Relu\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x: np.array):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "# 2.Sigmoid\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x: np.array):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        return dx\n",
    "    \n",
    "# 3.Affine\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x: np.array):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "        \n",
    "        out = np.matmul(self.x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.matmul(dout, self.W.T)\n",
    "        self.dW = np.matmul(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)\n",
    "        return dx\n",
    "\n",
    "# 4.Softmax-with-Loss\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x: np.array, t: np.array):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss  # 순전파 시, Loss 계층이 가장 마지막이니까 loss를 리턴\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "#===========================================================\n",
    "# -> dx는 입력 데이터의 변화량을 의미하는데, 입력 데이터는 변경시킬 수 없음! \n",
    "# -> 우리는 W,b만을 변경시켜야 하므로 마지막 gradient 메소드에서 반환되는 dout 변수는 파라미터 변화량 계산하기 위한 정의용인듯!\n",
    "#===========================================================\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        #================================================================================\n",
    "        #계층 생성 -> 순서가 있는 OrderedDict 객체 사용(Why?-> 순전파/역전파 호출 계층 순서가 반대기 떄문!)\n",
    "        #================================================================================\n",
    "        self.layers = OrderedDict()\n",
    "        # 1.Affine & Activtion(Relu)\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        # 2.Affine & Activation(Softmax)\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastlayer = SoftmaxWithLoss()\n",
    "        \n",
    "    # 순전파 시 예측\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "            \n",
    "    # 순전파 시 Softmax & 손실함수\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastlayer.forward(y, t) # 이떄, y는 layer거쳐 나온 출력값임!\n",
    "    \n",
    "    # 정확도 측정\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        # 정답도 One-hot 형태라면 레이블 형태로 변경\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1) \n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(y.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # 역전파\n",
    "    def gradient(self, x, t):\n",
    "        # 순전파 수행\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # 역전파 수행\n",
    "        dout = 1\n",
    "        dout = self.lastlayer.backward(dout)  # Softmax-with-Loss 계층 역전파\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse() # 레이어 순서 뒤집기\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 역전파 수행한 후의 파라미터 변화량 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    # 수치미분\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda w: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수치미분 활용해 오차역전파법으로 구한 기울기 값 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: W1 diff: 3.7018114837401005e-10\n",
      "key: b1 diff: 2.196205218836381e-09\n",
      "key: W2 diff: 4.65686156334598e-09\n",
      "key: b2 diff: 1.4011442090039461e-07\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "# model\n",
    "network = TwoLayerNet(input_size=28*28, hidden_size=50, output_size=10)\n",
    "\n",
    "# Batch\n",
    "X_batch = X_train[:3]\n",
    "y_batch = y_train[:3]\n",
    "\n",
    "# 수치미분\n",
    "grad_numerical = network.numerical_gradient(X_batch, y_batch)\n",
    "grad_propagation = network.gradient(X_batch, y_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.mean(np.abs(grad_numerical[key] - grad_propagation[key]))\n",
    "    print('key:', key, 'diff:', diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오차역전파를 사용한 MNIST 데이터 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 학습 후 Train Acc: 0.15\n",
      "1번째 학습 후 Test Acc: 0.098\n",
      "\n",
      "1001번째 학습 후 Train Acc: 0.95\n",
      "1001번째 학습 후 Test Acc: 0.922\n",
      "\n",
      "2001번째 학습 후 Train Acc: 0.97\n",
      "2001번째 학습 후 Test Acc: 0.94\n",
      "\n",
      "3001번째 학습 후 Train Acc: 0.99\n",
      "3001번째 학습 후 Test Acc: 0.95\n",
      "\n",
      "4001번째 학습 후 Train Acc: 0.97\n",
      "4001번째 학습 후 Test Acc: 0.957\n",
      "\n",
      "5001번째 학습 후 Train Acc: 0.95\n",
      "5001번째 학습 후 Test Acc: 0.961\n",
      "\n",
      "6001번째 학습 후 Train Acc: 0.98\n",
      "6001번째 학습 후 Test Acc: 0.966\n",
      "\n",
      "7001번째 학습 후 Train Acc: 1.0\n",
      "7001번째 학습 후 Test Acc: 0.966\n",
      "\n",
      "8001번째 학습 후 Train Acc: 0.99\n",
      "8001번째 학습 후 Test Acc: 0.967\n",
      "\n",
      "9001번째 학습 후 Train Acc: 0.99\n",
      "9001번째 학습 후 Test Acc: 0.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=28*28, hidden_size=50, output_size=10)\n",
    "\n",
    "steps = 10000\n",
    "train_size = X_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "# Mini-batch로 학습\n",
    "for i in range(steps):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    X_batch = X_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    # 오차역전파로 학습하면서 파라미터 변화량 get!\n",
    "    grad = network.gradient(X_batch, y_batch)\n",
    "    \n",
    "    # 얻은 변화량으로 SGD(경사 하강) 수행!\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # SGD 수행 후 다시 학습 데이터에 예측해서 Loss 계산\n",
    "    loss = network.loss(X_batch, y_batch)\n",
    "    train_loss.append(loss)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        tr_acc = network.accuracy(X_train[batch_mask], y_train[batch_mask])\n",
    "        te_acc = network.accuracy(X_test, y_test)\n",
    "        train_acc.append(tr_acc)\n",
    "        test_acc.append(te_acc)\n",
    "        print(f'{i+1}번째 학습 후 Train Acc:', round(tr_acc, 3))\n",
    "        print(f'{i+1}번째 학습 후 Test Acc:', round(te_acc, 3))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
