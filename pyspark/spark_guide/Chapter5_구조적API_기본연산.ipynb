{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tropical-prior",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#스키마\" data-toc-modified-id=\"스키마-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>스키마</a></span></li><li><span><a href=\"#칼럼과-표현식\" data-toc-modified-id=\"칼럼과-표현식-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>칼럼과 표현식</a></span><ul class=\"toc-item\"><li><span><a href=\"#칼럼\" data-toc-modified-id=\"칼럼-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>칼럼</a></span></li><li><span><a href=\"#표현식\" data-toc-modified-id=\"표현식-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>표현식</a></span></li><li><span><a href=\"#레코드와-로우\" data-toc-modified-id=\"레코드와-로우-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>레코드와 로우</a></span></li></ul></li><li><span><a href=\"#DataFrame의-트랜스포메이션\" data-toc-modified-id=\"DataFrame의-트랜스포메이션-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>DataFrame의 트랜스포메이션</a></span><ul class=\"toc-item\"><li><span><a href=\"#데이터프레임-생성\" data-toc-modified-id=\"데이터프레임-생성-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>데이터프레임 생성</a></span></li><li><span><a href=\"#트랜스포메이션-종류\" data-toc-modified-id=\"트랜스포메이션-종류-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>트랜스포메이션 종류</a></span><ul class=\"toc-item\"><li><span><a href=\"#select-&amp;-selectExpr\" data-toc-modified-id=\"select-&amp;-selectExpr-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span><code>select</code> &amp; <code>selectExpr</code></a></span></li></ul></li><li><span><a href=\"#스파크-데이터-타입으로-변환하기\" data-toc-modified-id=\"스파크-데이터-타입으로-변환하기-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>스파크 데이터 타입으로 변환하기</a></span></li><li><span><a href=\"#칼럼-추가하기\" data-toc-modified-id=\"칼럼-추가하기-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>칼럼 추가하기</a></span></li><li><span><a href=\"#칼럼명-변경하기\" data-toc-modified-id=\"칼럼명-변경하기-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>칼럼명 변경하기</a></span><ul class=\"toc-item\"><li><span><a href=\"#예약-문자와-키워드\" data-toc-modified-id=\"예약-문자와-키워드-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>예약 문자와 키워드</a></span></li></ul></li><li><span><a href=\"#대소문자-구분\" data-toc-modified-id=\"대소문자-구분-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>대소문자 구분</a></span></li><li><span><a href=\"#칼럼-제거하기\" data-toc-modified-id=\"칼럼-제거하기-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>칼럼 제거하기</a></span></li><li><span><a href=\"#칼럼의-데이터-타입-변경하기\" data-toc-modified-id=\"칼럼의-데이터-타입-변경하기-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>칼럼의 데이터 타입 변경하기</a></span></li><li><span><a href=\"#로우-필터링하기\" data-toc-modified-id=\"로우-필터링하기-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>로우 필터링하기</a></span></li><li><span><a href=\"#고유한(unique)-로우-얻기\" data-toc-modified-id=\"고유한(unique)-로우-얻기-3.10\"><span class=\"toc-item-num\">3.10&nbsp;&nbsp;</span>고유한(unique) 로우 얻기</a></span></li><li><span><a href=\"#무작위-샘플-만들기\" data-toc-modified-id=\"무작위-샘플-만들기-3.11\"><span class=\"toc-item-num\">3.11&nbsp;&nbsp;</span>무작위 샘플 만들기</a></span></li><li><span><a href=\"#임의-분할하기\" data-toc-modified-id=\"임의-분할하기-3.12\"><span class=\"toc-item-num\">3.12&nbsp;&nbsp;</span>임의 분할하기</a></span></li><li><span><a href=\"#로우-합치기와-추가하기\" data-toc-modified-id=\"로우-합치기와-추가하기-3.13\"><span class=\"toc-item-num\">3.13&nbsp;&nbsp;</span>로우 합치기와 추가하기</a></span></li><li><span><a href=\"#로우--정렬하기\" data-toc-modified-id=\"로우--정렬하기-3.14\"><span class=\"toc-item-num\">3.14&nbsp;&nbsp;</span>로우  정렬하기</a></span></li><li><span><a href=\"#로우-수-제한하기\" data-toc-modified-id=\"로우-수-제한하기-3.15\"><span class=\"toc-item-num\">3.15&nbsp;&nbsp;</span>로우 수 제한하기</a></span></li><li><span><a href=\"#repartition-&amp;-coalesce\" data-toc-modified-id=\"repartition-&amp;-coalesce-3.16\"><span class=\"toc-item-num\">3.16&nbsp;&nbsp;</span>repartition &amp; coalesce</a></span></li><li><span><a href=\"#드라이버로-로우-데이터-수집\" data-toc-modified-id=\"드라이버로-로우-데이터-수집-3.17\"><span class=\"toc-item-num\">3.17&nbsp;&nbsp;</span>드라이버로 로우 데이터 수집</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lucky-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = spark.read.format('json').load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/flight-data/json/2015-summary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inappropriate-volunteer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스키마 살펴보기\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-firewall",
   "metadata": {},
   "source": [
    "# 스키마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "academic-chamber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('json').load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/flight-data/json/2015-summary.json').schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-movement",
   "metadata": {},
   "source": [
    "- 스키마를 출력한 결과\n",
    "    * ``StructType``이라는 리스트 안에 각 칼럼별로 ``StructField``가 담겨져 있음\n",
    "    * 즉, 스키마 = 여러개의 ``StructField`` 타입 필드로 구성된 ``StructType`` 객체\n",
    "- 데이터 타입과 스키마의 데이터 타입이 불일치하면 스파크에서는 오류 발생\n",
    "- 스파크에서는 자체 데이터 타입을 사용하므로 사용하는 언어 API(ex.Python, R 등)의 데이터 타입을 사용할 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-rental",
   "metadata": {},
   "source": [
    "- 스키마 정의시 **메타 데이터**를 정의할 수도 있음\n",
    "    * **메타 데이터란, 해당 칼럼과 관련된 정보이며 추후 스파크의 머신러닝 라이브러리에서 사용함**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "informed-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임에 스키마를 만들고 적용하는 예제\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField('DEST_COUNTRY_NAME', StringType(), True),\n",
    "    StructField('ORIGIN_COUNTRY_NAME', StringType(), True),\n",
    "    StructField('count', LongType(), False, metadata={'hello': 'world'})\n",
    "])\n",
    "\n",
    "# 데이터 로드 시 스키마 정의\n",
    "df = spark.read.format('json').schema(myManualSchema)\\\n",
    "     .load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/flight-data/json/2015-summary.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-tract",
   "metadata": {},
   "source": [
    "# 칼럼과 표현식\n",
    "- 표현식으로 데이터프레임의 칼럼을 선택, 조작, 제거 가능\n",
    "- 스파크의 칼럼은 표현식을 사용해 레코드(Row) 단위로 계산한 값을 단순하게 나타내는 논리적인 구조\n",
    "- 칼럼의 실제값을 얻으려면 레코드(Row)가 필요하고 레코드를 얻으려면 데이터프레임이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-lobby",
   "metadata": {},
   "source": [
    "## 칼럼\n",
    "\n",
    "- 칼럼을 생성, 참조할 수 있는 방법은 여러가지가 있지만 ``col()``이나 ``column()``을 사용하는 것이 가장 간단. 소괄호 인자에는 ``칼럼명``을 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "proud-slide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'someColumnName'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "\n",
    "col('someColumnName')\n",
    "column('someColumnName')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-portfolio",
   "metadata": {},
   "source": [
    "- 이 때 해당 칼럼이 로드한 DataFrame에 있는지 여부는 알 수 없음. 알려면 **카탈로그**에 저장된 정보와 비교해야 함. 그런데 이 비교하는 단계는 **분석기**가 트랜스포메이션으로 구성된 논리적실행계획을 검증 전/후로 가는 단계에 걸쳐있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-posting",
   "metadata": {},
   "source": [
    "- 명시적 컬럼 참조. ``col()`` 사용. 이는 데이터프레임 조인 시 특정 칼럼을 참조하는 데 사용\n",
    "- 단, ``col()``을 사용해 명시적으로 컬럼을 정의하면 **분석기** 실행 단계에서 컬럼 확인 절차를 생략함!(그러므로 더 빨라지겠지..?)\n",
    "- Pyspark에서는 ``df['column_name']``을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "geographic-deployment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-migration",
   "metadata": {},
   "source": [
    "## 표현식\n",
    "- 표현식이란, 데이터프레임의 레코드의 여러값에 대한 **트랜스포메이션의 집합**\n",
    "- ``expr()``과 ``col()``로 특정 칼럼을 참조하는 것은 동일\n",
    "- 예를 들어, ``expr('SomeCol - 5')`` == ``col('someCol') - 5`` == ``expr(SomeCol') - 5``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dressed-whole",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'((((SomeCol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "expr(\"(((SomeCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-rhythm",
   "metadata": {},
   "source": [
    "- 위 데이터프레임 코드는 SQL의 SELECT 구문으로 해당 표현식을 동일하게 사용해도 동일한 결과를 생성. 왜냐하면 실행 시점에 동일한 논리 트리로 컴파일되기 때문. 성능도 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "moved-royal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임의 컬럼에 접근해보기\n",
    "spark.read.format('json').load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/flight-data/json/2015-summary.json')\\\n",
    "     .columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-format",
   "metadata": {},
   "source": [
    "## 레코드와 로우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "according-mapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 첫 번째 레코드(Row 객체)를 반환\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-writer",
   "metadata": {},
   "source": [
    "- Row객체인 레코드를 생성해보자\n",
    "- 단, Row는 스키마를 갖고 있지 않고 데이터프레임만 스키마를 가짐. 따라서 Row 생성 시, 데이터프레임의 명시된 스키마 순서와 동일하게 해주어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "environmental-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "myRow = Row('Hello', None, 1, True) # 마지막 False는 nullable에 대한 인자!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "becoming-zealand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello None\n"
     ]
    }
   ],
   "source": [
    "# 로우의 데이터에 접근하기\n",
    "print(myRow[0], myRow[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-allergy",
   "metadata": {},
   "source": [
    "# DataFrame의 트랜스포메이션\n",
    "## 데이터프레임 생성\n",
    "- 기본 트렌스포메이션 결과를 확인하기 위해 임시 뷰로 테이블 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "gothic-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('json').load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/flight-data/json/2015-summary.json')\n",
    "df.createOrReplaceTempView('dfTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "secure-permission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afraid-wonder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|null|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField('some', StringType(), True),\n",
    "    StructField('col', StringType(), True),\n",
    "    StructField('names', LongType(), False)\n",
    "])\n",
    "\n",
    "myRow = Row(\"Hello\", None, 1) # 데이터프레임이 갖는 Schema 순서로 정의\n",
    "myDf = spark.createDataFrame([myRow], schema=myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-causing",
   "metadata": {},
   "source": [
    "## 트랜스포메이션 종류\n",
    "- 칼럼이나 표현식을 사용하는 ``select``\n",
    "- 문자열 표현식을 사용하는 ``selectExpr``\n",
    "- 매서드로 사용할 수 없는 ``org.apache.spark.sql.functions`` 패키지에 포함된 다양한 함수\n",
    "<br><br>\n",
    "- 위 3가지의 방법으로 데이터프레임을 다룰 때 필요한 대부분의 트랜스포메이션을 수행할 수 있음\n",
    "### ``select`` & ``selectExpr``\n",
    "- 데이터 테이블에 SQL 쿼리문을 실행하는 것처럼 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "white-qualification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('DEST_COUNTRY_NAME').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "revised-indicator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 칼럼 가져오기\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "special-applicant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 다음과 같이 다양한 방법들로 칼럼을 참조할 수 있음\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "\n",
    "df.select(expr(\"DEST_COUNTRY_NAME\"),\n",
    "         col(\"DEST_COUNTRY_NAME\"),\n",
    "         column(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "affiliated-white",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+\n",
      "|    United States|    United States|\n",
      "|    United States|    United States|\n",
      "|    United States|    United States|\n",
      "|            Egypt|            Egypt|\n",
      "|    United States|    United States|\n",
      "+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 위와 같이 col(expr/column) 객체와 단순한 문자열을 함께 섞어써도 에러 발생하지 않음(책에선 에러가 발생한다고 했으나...)\n",
    "df.select(col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-stress",
   "metadata": {},
   "source": [
    "- 위 방법 중 ``expr``은 가장 유연하게 칼럼을 참조하는 방법\n",
    "    - ``AS(일종의 alias)``를 사용해 칼럼 이름을 재정의 가능\n",
    "    - ``alias``한 것을 다시 ``alias()`` 메소드로 원래대로 변경 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "vulnerable-camera",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  Destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS Destination\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "clean-geometry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS Destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-bandwidth",
   "metadata": {},
   "source": [
    "- 위와 같이 ``select()``안에 ``expr()``을 같이 사용하는 것을 효율적이고 간단하게 할 수 있는 것이 ``selectExpr``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "intimate-anthropology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|  Destination|count|\n",
      "+-------------+-----+\n",
      "|United States|   15|\n",
      "|United States|    1|\n",
      "|United States|  344|\n",
      "+-------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"DEST_COUNTRY_NAME AS Destination\", \"count\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-specific",
   "metadata": {},
   "source": [
    "- ``selectExpr``은 새로운 데이터프레임을 생성하는(ex.새로운 칼럼을 만들어내는 트렌스포메이션을 지정하면서 새로운 데이터프레임이 생성됨) 복잡한 표현식을 간단하게 만드는 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sixth-brooks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "|            Egypt|      United States|   15|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 출발지와 도착지가 같은지 나타내는 새로운 칼럼 만들기\n",
    "df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) AS withinCountry\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "hazardous-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+\n",
      "| avg(count)|count(DEST_COUNTRY_NAME)|\n",
      "+-----------+------------------------+\n",
      "|1770.765625|                     256|\n",
      "+-----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 간단한 집계(평균, 최댓값, 최소값 등) 가능\n",
    "df.selectExpr(\"AVG(count)\", \"COUNT(DEST_COUNTRY_NAME)\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-browse",
   "metadata": {},
   "source": [
    "## 스파크 데이터 타입으로 변환하기\n",
    "- 명시적인 값(상숫값 또는 추후 비교에 사용할 Scala값 같은 것들)을 스파크에 전달해주기 위해 스파크 데이터 타입으로 변환해야 함\n",
    "- 이 때, 리터럴(``lit()``)을 사용함. 리터럴은 스파크가 이해할 수 있는 값(스파크 데이터 타입)으로 변환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "original-punch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.select(col(\"*\"), lit(1).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-minutes",
   "metadata": {},
   "source": [
    "## 칼럼 추가하기\n",
    "- DataFrame의 ``withColumn(\"새로운칼럼이름\", 표현식)`` 메서드를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecological-diamond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "civic-exclusive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|Binary|\n",
      "+-----------------+-------------------+-----+------+\n",
      "|    United States|            Romania|   15| false|\n",
      "|    United States|            Croatia|    1| false|\n",
      "|    United States|            Ireland|  344| false|\n",
      "|            Egypt|      United States|   15| false|\n",
      "|    United States|              India|   62| false|\n",
      "+-----------------+-------------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 출발지와 도착지가 같은지 여부를 불리언 타입으로 하는 새로운 칼럼 만들기\n",
    "df.withColumn(\"Binary\", expr(\"DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "powered-absolute",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME`' given input columns: [DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count];;\n'Project [DEST_COUNTRY_NAME#55, ORIGIN_COUNTRY_NAME#56, count#57L, 'DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME AS binary#314]\n+- Relation[DEST_COUNTRY_NAME#55,ORIGIN_COUNTRY_NAME#56,count#57L] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-501b0fb11960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2094\u001b[0m         \"\"\"\n\u001b[1;32m   2095\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME`' given input columns: [DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count];;\n'Project [DEST_COUNTRY_NAME#55, ORIGIN_COUNTRY_NAME#56, count#57L, 'DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME AS binary#314]\n+- Relation[DEST_COUNTRY_NAME#55,ORIGIN_COUNTRY_NAME#56,count#57L] json\n"
     ]
    }
   ],
   "source": [
    "# col 사용시 에러 발생\n",
    "df.withColumn(\"binary\", col(\"DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-roller",
   "metadata": {},
   "source": [
    "- ``withColumn``은 칼럼명을 rename할 수도 있음. 그러나 어떻게 보면 동일한 칼럼이 중복적으로 발생하게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fresh-clothing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'DEST_COUNTRY_NAME2']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"DEST_COUNTRY_NAME2\", col(\"DEST_COUNTRY_NAME\")).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-vaccine",
   "metadata": {},
   "source": [
    "## 칼럼명 변경하기\n",
    "- ``withColumnRenamed(\"원래칼럼명\", \"새로운칼럼명\")``로 변경 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "balanced-portfolio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----+\n",
      "|  Destination|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------+-------------------+-----+\n",
      "|United States|            Romania|   15|\n",
      "|United States|            Croatia|    1|\n",
      "+-------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"Destination\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-bangkok",
   "metadata": {},
   "source": [
    "### 예약 문자와 키워드\n",
    "- 공백이나 하이픈(-)같은 예약 문자는 칼럼명에 포함시킬 수 없음. 만약 사용하려면 백틱( ` )을 사용해 이스케이핑해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "matched-jacket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|This Long Column-Name|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "|    United States|            Romania|   15|              Romania|\n",
      "|    United States|            Croatia|    1|              Croatia|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 이스케이핑이 필요하지 않은 경우 -> withColumn 첫 번째 인자에 새로운 칼럼명을 넣을 때\n",
    "dfWithLongColName = df.withColumn(\"This Long Column-Name\", expr(\"ORIGIN_COUNTRY_NAME\"))\n",
    "dfWithLongColName.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "hourly-member",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|This Long Column-Name|New Col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 이스케이핑이 필요한 경우 -> 참조하는 표현식에 예약 문자가 있기 때문에 백틱 사용\n",
    "dfWithLongColName.selectExpr(\"`This Long Column-Name`\",\n",
    "                            \"`This Long Column-Name` as `New Col`\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "hispanic-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|This Long Column-Name|This Long Column-Name|\n",
      "+---------------------+---------------------+\n",
      "|              Romania|              Romania|\n",
      "|              Croatia|              Croatia|\n",
      "+---------------------+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 조회할 때도 칼럼 명에 예악문자나 키워드가 포함되어 있다면 백틱 추가해서 조회!\n",
    "dfWithLongColName.select(expr(\"`This Long Column-Name`\"), col(\"`This Long Column-Name`\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-slovenia",
   "metadata": {},
   "source": [
    "## 대소문자 구분\n",
    "- 기본적으로 스파크는 대소문자를 구분하지 않음. 하지만 지정해서 대소문자를 구분하게 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "complete-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark\n",
    "spark.conf.set('spark.sql.caseSensitive', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-overview",
   "metadata": {},
   "source": [
    "## 칼럼 제거하기\n",
    "- ``select``로 제거(필요없는 칼럼영 빼고 조회)할 수도 있지만 ``drop``으로도 가능\n",
    "- ``drop``인자에 여러개의 칼럼명을 넣어 다수의 칼럼들을 제거 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "collect-commerce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "angry-proportion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop multiple columns\n",
    "dfWithLongColName.drop(\"count\", \"This Long Column-Name\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-region",
   "metadata": {},
   "source": [
    "## 칼럼의 데이터 타입 변경하기\n",
    "- ``cast``메서드 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "artificial-bleeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- count2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# int형 -> string으로\n",
    "ex_df = df.withColumn(\"count2\", col(\"count\").cast(\"string\"))\n",
    "ex_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-beginning",
   "metadata": {},
   "source": [
    "## 로우 필터링하기\n",
    "- True/False를 판별하는 표현식을 만들어 False인 로우들을 걸러내기\n",
    "- ``where``과 ``filter`` 사용(두 방법 모두 같은 연산을 수행하며 같은 파라미터 타입을 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "meaningful-witness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# where\n",
    "df.where(\"count < 2\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "pediatric-classic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "df.filter(\"count < 2\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-worse",
   "metadata": {},
   "source": [
    "- 필터링 조건을 여러개 붙일 수 있음. 그러나 스파크는 모든 필터링 작업을 동시에 수행하기 때문에 필터를 여러개 지정하되 필터링 순서는 스파크의 판단에 맡겨야 함\n",
    "    * 그러므로 여러개의 조건들 사이에 시간 순서의 종속성이 존재하면 안 되며 조건 끼리는 독립성이 존재해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "corporate-player",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-james",
   "metadata": {},
   "source": [
    "## 고유한(unique) 로우 얻기\n",
    "- ``distinct`` 메서드 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cooked-origin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 출발지, 도착지가 고유한 데이터\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-honor",
   "metadata": {},
   "source": [
    "- 출발지 -> 도착지가 유니크한 값들이 256개나 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "anticipated-theta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-armenia",
   "metadata": {},
   "source": [
    "## 무작위 샘플 만들기\n",
    "- 데이터프레임의 ``sample`` 메서드 사용\n",
    "- 표본 데이터 추출 비율을 지정할 수 있으며 복원추출 또는 비복원 추출을 지정할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "imposed-title",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "\n",
    "df.sample(withReplacement=withReplacement, fraction=fraction, seed=seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-organic",
   "metadata": {},
   "source": [
    "## 임의 분할하기\n",
    "- 보통 Train/Test 데이터셋 분할시 자주 사용\n",
    "- ``randomSplit`` 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "proper-architecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint], DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]] \n",
      " <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "dataFrames = df.randomSplit([0.25, 0.75], seed=42)\n",
    "print(dataFrames,'\\n', type(dataFrames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "herbal-granny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataFrames[0].count(), dataFrames[1].count())\n",
    "dataFrames[0].count() > dataFrames[1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-method",
   "metadata": {},
   "source": [
    "## 로우 합치기와 추가하기\n",
    "- 데이터프레임은 불변성이기 때문에 로우를 추가하려면 추가할 로우를 새로운 데이터프레임으로 만들고 기존의 데이터프레임과 결합(``union``)하는 방향으로 가야 함\n",
    "- 결합할 시, 반드시 기존의 데이터프레임의 스키마와 칼럼 수가 동일해야 함!\n",
    "- 참고로 ``union``은 현재 스키마가 아닌 칼럼 위치를 기반으로 동작하기 때문에 사용자가 생각한 대로 칼렴들이 자동 정렬되어 있지 않을 수 있음!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "complex-payroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 새로운 데이터프레임 만들기 위해 Row 객체를 만들어서 RDD 만들어야 함\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = df.schema  # 기존의 데이터프레임의 스키마\n",
    "\n",
    "# 책 속 'L'은 Long Integer Type을 따로 정의해야 하는 Python 2.X 버전이라 그럼\n",
    "newRows = [Row('New Country', 'Other Country', 5),\n",
    "          Row('New Country 2', 'Other Country 3', 1)]\n",
    "parallelizeRows = spark.sparkContext.parallelize(newRows)  # RDD로 변환\n",
    "newDF = spark.createDataFrame(parallelizeRows, schema=schema)  # DF으로 변환\n",
    "\n",
    "# 기존DF.union(새로운DF) ~ \n",
    "df.union(newDF)\\\n",
    "  .where('count = 1')\\\n",
    "  .where(col('ORIGIN_COUNTRY_NAME') != 'United States')\\\n",
    "  .where(col('DEST_COUNTRY_NAME') != 'United States')\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-rotation",
   "metadata": {},
   "source": [
    "## 로우  정렬하기\n",
    "- ``sort``, ``orderBy`` 메서드를 사용\n",
    "- ``orderBy`` 내부 코드에서 ``sort``를 사용하기 때문에 두 메소드 모두 동일한 방식으로 동작\n",
    "- 디폴트는 오름차순(ASC) 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "unauthorized-voltage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('count').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "vertical-expression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여러가지 정렬 기준 가능\n",
    "df.orderBy('count', 'DEST_COUNTRY_NAME').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "loaded-holder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# col() 메소드를 사용해서도 정렬 기준 제시 가능\n",
    "df.orderBy(col('count'), col('DEST_COUNTRY_NAME')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-aquarium",
   "metadata": {},
   "source": [
    "- 정렬 기준을 명확히 지정하려면 ``asc``, ``desc`` 메소드를 임포트해서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "sophisticated-watershed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc, desc\n",
    "\n",
    "# expr 메소드를 통해서 표현식으로 정렬 기준 제시\n",
    "df.orderBy(expr(\"count\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-detail",
   "metadata": {},
   "source": [
    "- ``df.orderBy(expr(\"count desc\")).show(5)``\n",
    "- 위와 같이 표현하면 ``desc``가 ``count``의 alias로 간주함. 따라서 정렬이 안됨. 그러므로 아래의 방법들을 사용하기\n",
    "    * 관련 <a href='https://stackoverflow.com/questions/53772271/apache-spark-2-0-expression-string-to-orderby-sort-column-in-descending-o'>stackoverflow</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "treated-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('count', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "statutory-lindsay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col('count').desc(), col('DEST_COUNTRY_NAME').asc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-future",
   "metadata": {},
   "source": [
    "- ``asc_nulls_first``, ``desc_nulls_first``, ``asc_nulls_last``, ``desc_nulls_last``로 DF에서 ``Null``값이 표시되는 기준을 지정할 수 있음(``Null``값이 처음/마지막에 오도록 설정 가능)\n",
    "<br><br>\n",
    "- 또한 트랜스포메이션을 처리하기 이전에 성능을 최적화하기 위해 **파티션별 정렬을 수행**하기도 함. 해당 정렬은 ``sortWithinPartitions``메서드로 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "developing-powder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('json').load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/flight-data/json/2015-summary.json')\\\n",
    "     .sortWithinPartitions('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-wallace",
   "metadata": {},
   "source": [
    "## 로우 수 제한하기\n",
    "- ``limit`` 메서드 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "promising-skill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "+-----------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col('count'), ascending=False).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-macro",
   "metadata": {},
   "source": [
    "## repartition & coalesce\n",
    "- 물리적 데이터 구성을 제어함으로써 최적화 기법\n",
    "- ``repartition`` 호출 시 전체 데이터를 셔플\n",
    "    * 향후 사용할 파티션 수 > 현재 파티션 수\n",
    "    * 칼럼을 기준으로 파티션을 만드는 경우\n",
    "    * 위 2가지의 경우에만 사용\n",
    "    \n",
    "- 자주 필터링하는 칼럼을 기준으로 파티션을 재분배 즉, 데이터를 재분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "pressing-million",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions() # 현재 파티션 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "outside-ferry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파티션 5개로 데이터를 재분할\n",
    "test = df.repartition(5)\n",
    "test.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "automated-villa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자주사용하는 칼럼을 기준으로 파티션 재분할\n",
    "test2 = df.repartition(col('DEST_COUNTRY_NAME'))\n",
    "test2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aggressive-burden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자주 사용하는 칼럼과 재분할 시킬 파티션 개수 정의 가능\n",
    "test3 = df.repartition(5, col('DEST_COUNTRY_NAME'))\n",
    "test3.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-slovak",
   "metadata": {},
   "source": [
    "- ``coalesce`` 메서드는 파티션을 **줄일 때!** 사용! 즉, 데이터에 셔플을 가하지 않고 파티션을 병합하기 위함!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "about-parts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파티션을 5개로 분할시키고 다시 2개의 파티션으로 병합하는 경우\n",
    "test4 = df.repartition(5, col('DEST_COUNTRY_NAME')).coalesce(2)\n",
    "test4.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-improvement",
   "metadata": {},
   "source": [
    "## 드라이버로 로우 데이터 수집\n",
    "- 스파크는 드라이버에서 클러스터 상태 정보를 갖고 있음\n",
    "- **로컬 환경에서 데이터를 다루려면 드라이버로 데이터를 수집**해야 함!(아직, 드라이버로 데이터를 수집하는 연산은 안배움)\n",
    "- 연산 중 ``collect``는 전체 DF의 모든 데이터를 수집하며 ``take``는 상위 N개의 로우를 반환, ``show``는 여러 로우를 보기 좋게 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "sunset-genome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # 상위 5개의 로우를 반환\n",
    "collectDF.show(5, truncate=True)\n",
    "collectDF.collect() # 전체 DF의 모든 데이터를 수집\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-protection",
   "metadata": {},
   "source": [
    "- 전체 데이터셋에 대한 반복(iteration)처리를 위해 드라이버로 로우를 모으는 또 다른 방법: ``toLocalIterator``\n",
    "    * 해당 메서드는 이터레이터로 모든 파티션의 데이터를 드라이버에 전달\n",
    "    * 해당 메서드로 데이터셋의 파티션을 차례로 하나씩 반복처리 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "adequate-staff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _local_iterator_from_socket.<locals>.PyLocalIterable.__iter__ at 0x7fad4f858950>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.toLocalIterator() # generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "needed-investigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)\n",
      "Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62)\n",
      "Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588)\n",
      "Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40)\n",
      "Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)\n"
     ]
    }
   ],
   "source": [
    "for i in collectDF.toLocalIterator():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "expected-capability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n"
     ]
    }
   ],
   "source": [
    "print(next(collectDF.toLocalIterator()))\n",
    "print(next(collectDF.toLocalIterator()))\n",
    "print(next(collectDF.toLocalIterator()))\n",
    "print(next(collectDF.toLocalIterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-marker",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
