{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7302bab2",
   "metadata": {
    "papermill": {
     "duration": 0.036697,
     "end_time": "2021-10-12T03:48:53.762226",
     "exception": false,
     "start_time": "2021-10-12T03:48:53.725529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. 일반적인 Tensorflow.keras CNN 모델링\n",
    "- ``cifar10`` 데이터로 실습 테스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2850ccc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T03:48:53.919698Z",
     "iopub.status.busy": "2021-10-12T03:48:53.908484Z",
     "iopub.status.idle": "2021-10-12T03:49:03.151544Z",
     "shell.execute_reply": "2021-10-12T03:49:03.151975Z",
     "shell.execute_reply.started": "2021-10-03T04:28:46.49443Z"
    },
    "papermill": {
     "duration": 9.354334,
     "end_time": "2021-10-12T03:49:03.152258",
     "exception": false,
     "start_time": "2021-10-12T03:48:53.797924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-12 03:48:54.406165: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2s 0us/step\n",
      "Train: (42500, 32, 32, 3) (42500, 10)\n",
      "Valid: (7500, 32, 32, 3) (7500, 10)\n",
      "Test: (10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_datasets():\n",
    "    (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "    \n",
    "def scaled_pixels(images, labels, scaling=False):\n",
    "    if scaling:\n",
    "        images = np.array(images/255.0, dtype=np.float32)\n",
    "    else:\n",
    "        images = np.array(images, dtype=np.float32)\n",
    "    labels = np.array(labels, dtype=np.float32)\n",
    "    return images, labels\n",
    "\n",
    "def transform_ohe(labels):\n",
    "    ohe_labels = to_categorical(labels)\n",
    "    return ohe_labels\n",
    "\n",
    "def train_valid_split(train_images, train_ohe_labels):\n",
    "    tr_images, val_images, tr_ohe_labels, val_ohe_labels = train_test_split(train_images, train_ohe_labels,\n",
    "                                                                           test_size=0.15, random_state=42)\n",
    "    return tr_images, val_images, tr_ohe_labels, val_ohe_labels\n",
    "\n",
    "def preprocess_data():\n",
    "    train_images, train_labels, test_images, test_labels = load_datasets()\n",
    "    train_images, train_labels = scaled_pixels(train_images, train_labels, scaling=False)\n",
    "    test_images, test_labels = scaled_pixels(test_images, test_labels, scaling=False)\n",
    "    train_ohe_labels = transform_ohe(train_labels)\n",
    "    test_ohe_labels = transform_ohe(test_labels)\n",
    "    tr_images, val_images, tr_ohe_labels, val_ohe_labels = train_valid_split(train_images, train_ohe_labels)\n",
    "    print('Train:', tr_images.shape, tr_ohe_labels.shape)\n",
    "    print('Valid:', val_images.shape, val_ohe_labels.shape)\n",
    "    print('Test:', test_images.shape, test_ohe_labels.shape)\n",
    "    return tr_images, val_images, tr_ohe_labels, val_ohe_labels, test_images, test_ohe_labels\n",
    "\n",
    "tr_images, val_images, tr_ohe_labels, val_ohe_labels, test_images, test_ohe_labels = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cdd1759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T03:49:03.247903Z",
     "iopub.status.busy": "2021-10-12T03:49:03.246972Z",
     "iopub.status.idle": "2021-10-12T03:49:49.017418Z",
     "shell.execute_reply": "2021-10-12T03:49:49.015624Z",
     "shell.execute_reply.started": "2021-10-04T04:29:23.653081Z"
    },
    "papermill": {
     "duration": 45.818614,
     "end_time": "2021-10-12T03:49:49.017783",
     "exception": true,
     "start_time": "2021-10-12T03:49:03.199169",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-12 03:49:03.282832: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-12 03:49:03.286171: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-10-12 03:49:03.333205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 03:49:03.333919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2021-10-12 03:49:03.333994: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-12 03:49:03.361411: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-12 03:49:03.361605: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-12 03:49:03.378687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-12 03:49:03.387440: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-12 03:49:03.413148: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-12 03:49:03.421322: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-10-12 03:49:03.423976: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-10-12 03:49:03.424208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 03:49:03.425252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 03:49:03.427057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-12 03:49:03.428215: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-12 03:49:03.428456: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-12 03:49:03.428616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 03:49:03.429455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2021-10-12 03:49:03.429514: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-12 03:49:03.429551: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-12 03:49:03.429576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-12 03:49:03.429600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-12 03:49:03.429623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-12 03:49:03.429646: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-12 03:49:03.429669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-10-12 03:49:03.429692: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-10-12 03:49:03.429794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 03:49:03.430706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 03:49:03.431509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-12 03:49:03.432581: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-12 03:49:04.995159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-10-12 03:49:04.995205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-10-12 03:49:04.995214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-10-12 03:49:04.997607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 03:49:04.998359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 03:49:04.998967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-12 03:49:04.999530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "2021-10-12 03:49:06.371029: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-12 03:49:06.382077: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000170000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-12 03:49:07.052867: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-12 03:49:07.809140: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-12 03:49:07.834020: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665/665 [==============================] - 15s 12ms/step - loss: 3.1988 - accuracy: 0.1615 - val_loss: 1.6863 - val_accuracy: 0.4031\n",
      "Epoch 2/20\n",
      "665/665 [==============================] - 7s 10ms/step - loss: 1.7522 - accuracy: 0.3567 - val_loss: 1.4365 - val_accuracy: 0.4884\n",
      "Epoch 3/20\n",
      "665/665 [==============================] - 7s 10ms/step - loss: 1.5514 - accuracy: 0.4371 - val_loss: 1.3162 - val_accuracy: 0.5272\n",
      "Epoch 4/20\n",
      "665/665 [==============================] - 7s 11ms/step - loss: 1.4079 - accuracy: 0.4976 - val_loss: 1.1640 - val_accuracy: 0.5885\n",
      "Epoch 5/20\n",
      "665/665 [==============================] - 7s 10ms/step - loss: 1.2690 - accuracy: 0.5518 - val_loss: 1.1125 - val_accuracy: 0.6156\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = '/kaggle/working/models/weights.05-1.11.hdf5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25/2470722029.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# 모델 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m train_hist = model.fit(x=tr_images, y=tr_ohe_labels, batch_size=BATCH_SIZE, epochs=20, validation_data=(val_images, val_ohe_labels),\n\u001b[0;32m---> 48\u001b[0;31m                       callbacks=[mc_call, es_call, lr_call])\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;31m# 모델 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mtest_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ohe_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_should_save_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1416\u001b[0m                         'directory: {}'.format(filepath))\n\u001b[1;32m   1417\u001b[0m         \u001b[0;31m# Re-throw the error for any other causes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_file_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1392\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m                 self.model.save_weights(\n\u001b[0;32m-> 1394\u001b[0;31m                     filepath, overwrite=True, options=self._options)\n\u001b[0m\u001b[1;32m   1395\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[1;32m   2105\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2107\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2108\u001b[0m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '/kaggle/working/models/weights.05-1.11.hdf5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "# 내가 직접 CNN 모델 만들어 모델링\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "def create_model(input_size, verbose=False):\n",
    "    input_tensor = Input(shape=(input_size, input_size, 3))\n",
    "    x = Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')(input_tensor)\n",
    "    x = Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=2)(x)\n",
    "    \n",
    "    x = Conv2D(filters=128, kernel_size=3, padding='valid', activation='relu')(x)\n",
    "    x = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=2)(x)\n",
    "    # Classfier Layer\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=512, activation='relu')(x)\n",
    "    x = Dropout(rate=0.4)(x)\n",
    "    x = Dense(units=64, activation='relu')(x)\n",
    "    x = Dropout(rate=0.3)(x)\n",
    "    output = Dense(units=10, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "INPUT_SIZE = tr_images.shape[1]\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 모델 정의\n",
    "model = create_model(input_size=INPUT_SIZE, verbose=False)\n",
    "# 모델 compile\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# callbacks\n",
    "mc_call = ModelCheckpoint(filepath='/kaggle/working/models/weights.{epoch:02d}-{val_loss:.02f}.hdf5', monitor='val_loss', mode='min',\n",
    "                         save_best_only=True, save_weights_only=True, period=5, verbose=0)\n",
    "es_call = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n",
    "lr_call = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.2, patience=4, verbose=1)\n",
    "# 모델 학습\n",
    "train_hist = model.fit(x=tr_images, y=tr_ohe_labels, batch_size=BATCH_SIZE, epochs=20, validation_data=(val_images, val_ohe_labels),\n",
    "                      callbacks=[mc_call, es_call, lr_call])\n",
    "# 모델 평가\n",
    "test_hist = model.evaluate(x=test_images, y=test_ohe_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed229003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T04:49:51.689977Z",
     "iopub.status.busy": "2021-10-03T04:49:51.689381Z",
     "iopub.status.idle": "2021-10-03T04:49:51.74016Z",
     "shell.execute_reply": "2021-10-03T04:49:51.739479Z",
     "shell.execute_reply.started": "2021-10-03T04:49:51.68994Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 테스트 데이터 하나로 predict 하기 -> 한 개 예측해도 배치 사이즈 포함해서 4차원 shape으로 정의해주어야 함\n",
    "single_img = np.reshape(test_images[0], (1, INPUT_SIZE, INPUT_SIZE, 3))\n",
    "print('single_img shape:', single_img.shape)\n",
    "\n",
    "# 위 모델에서 y값이 원-핫 인코딩 상태이기 때문에 -> 예측 결과도 원-핫 인코딩 형태로 나옴 -> argmax 해주어야 함\n",
    "prediction = model.predict(x=single_img, verbose=1)\n",
    "print('prediction 결과:', prediction.shape)\n",
    "final_prediction = np.argmax(prediction, axis=1)\n",
    "print('final_prediction 결과 클래스:', final_prediction, final_prediction[0])\n",
    "\n",
    "# 모델 메모리에서 초기화\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254332de",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2. Tensorflow.keras - ImageDataGenerator 파이프라인 활용\n",
    "- ``flow()`` -> 이미지를 Numpy array 형태로 받아올 수 있을 때\n",
    "- ``flow_from_directory()`` -> 이미지, 레이블을 디렉토리 경로 형태로 받아올 수 있을 때\n",
    "- ``flow_from_dataframe()`` -> 이미지, 레이블을 디렉토리 경로로 받아올 수 있는데, 이 메타 데이터들을 pandas.dataframe에 저장할 수 있을 때"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f215499",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2-1. ``flow()``\n",
    "- ``cifar10`` 데이터를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d2c86",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-10-03T04:59:08.340041Z",
     "iopub.status.busy": "2021-10-03T04:59:08.33977Z",
     "iopub.status.idle": "2021-10-03T05:01:48.872055Z",
     "shell.execute_reply": "2021-10-03T05:01:48.871371Z",
     "shell.execute_reply.started": "2021-10-03T04:59:08.340014Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ImageGenerator 데이터 유형에 맞게 정의 -> 적용할 증강 기법 정의\n",
    "tr_gen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rescale=1/255.)\n",
    "val_gen = ImageDataGenerator(rescale=1/255.)\n",
    "test_gen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "# flow 메소드로 Numpy Array Iterator 만들기 -> 여기서 X, y, 배치 사이즈, 셔플 유무 결정\n",
    "tr_gen_flow = tr_gen.flow(x=tr_images, y=tr_ohe_labels, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_gen_flow = val_gen.flow(x=val_images, y=val_ohe_labels, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_gen_flow = test_gen.flow(x=test_images, y=test_ohe_labels, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 정의\n",
    "model = create_model(input_size=INPUT_SIZE, verbose=False)\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# callbacks\n",
    "es_call = EarlyStopping(monitor='val_loss', mode='min', patience=4, verbose=1)\n",
    "lr_call = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=3, verbose=1)\n",
    "# 모델 학습\n",
    "train_hist = model.fit(tr_gen_flow, epochs=20, validation_data=val_gen_flow, callbacks=[es_call, lr_call])\n",
    "# 모델 평가\n",
    "test_hist = model.evaluate(test_gen_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48058b58",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2-2. ``flow_from_directory()``\n",
    "- 캐글 오픈 데이터 <a href='https://www.kaggle.com/tongpython/cat-and-dog'>cat-and-dog</a> 데이터 활용\n",
    "- 인자로 넣어주는 디렉토리 경로의 바로 하위 디렉토리를 레이블, 그 밑의 이미지를 그 레이블에 해당하는 이미지로 추측하여 파이프라인 생성\n",
    "    - 단, 레이블과 이미지 매칭안시켜주면 난장판 발생..\n",
    "    - 또, Train/Valid 구분 안되어 있으면 이를 인위적으로 구분해주어야 하는데 매우 귀찮은 전처리 상황 발생.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763e835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T04:24:06.029871Z",
     "iopub.status.busy": "2021-10-04T04:24:06.029528Z",
     "iopub.status.idle": "2021-10-04T04:24:06.04087Z",
     "shell.execute_reply": "2021-10-04T04:24:06.039849Z",
     "shell.execute_reply.started": "2021-10-04T04:24:06.029839Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델은 전이학습 모델 사용 mobilenetv2\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def create_mobilenet(input_size, verbose=False):\n",
    "    input_tensor = Input(shape=(input_size, input_size, 3))\n",
    "    base_model = MobileNetV2(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "    base_output = base_model.output\n",
    "    # Classfier layer\n",
    "    x = GlobalAveragePooling2D()(base_output)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    output = Dense(units=1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc26d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T05:20:03.171568Z",
     "iopub.status.busy": "2021-10-03T05:20:03.171307Z",
     "iopub.status.idle": "2021-10-03T05:22:46.504518Z",
     "shell.execute_reply": "2021-10-03T05:22:46.503807Z",
     "shell.execute_reply.started": "2021-10-03T05:20:03.171541Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "INPUT_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# ImageGenerator 데이터 유형에 맞게 정의 -> 적용할 증강 기법 정의\n",
    "tr_gen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rescale=1/255.)\n",
    "test_gen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "# Numpy Array Iterator 생성\n",
    "directory = '/kaggle/input/cat-and-dog/'\n",
    "tr_gen_flow = tr_gen.flow_from_directory(directory=directory, target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "                                        class_mode='binary', batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_gen_flow = test_gen.flow_from_directory(directory=directory, target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "                                             class_mode='binary', batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 정의\n",
    "model = create_mobilenet(input_size=INPUT_SIZE, verbose=False)\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# callbacks\n",
    "mc_call = ModelCheckpoint(filepath='/kaggle/working/models/weights.{epoch:02d}-{loss:.02f}.hdf5',\n",
    "                         monitor='loss', mode='min', verbose=1, period=1)\n",
    "es_call = EarlyStopping(monitor='loss', mode='min', patience=4, verbose=1)\n",
    "lr_call = ReduceLROnPlateau(monitor='loss', mode='min', patience=3, verbose=1)\n",
    "# 모델 학습\n",
    "train_hist = model.fit(tr_gen_flow, epochs=2, callbacks=[es_call, lr_call, mc_call])\n",
    "# 모델 평가\n",
    "test_hist = model.evaluate(test_gen_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b1b64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T05:26:45.888805Z",
     "iopub.status.busy": "2021-10-03T05:26:45.888517Z",
     "iopub.status.idle": "2021-10-03T05:27:29.707367Z",
     "shell.execute_reply": "2021-10-03T05:27:29.706665Z",
     "shell.execute_reply.started": "2021-10-03T05:26:45.888775Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 저장한 모델 중 최적의 파라미터 모델 가중치(hdf5 파일)로드해서 test 데이터로 재평가\n",
    "# 1.위에서 저장할때 사용한 모델 아키텍처 동일하게 설계\n",
    "# 2.load_weights로 가중치 로드 후 컴파일\n",
    "# 3.test 데이터에 평가 및 예측\n",
    "\n",
    "optimal_model = create_mobilenet(INPUT_SIZE, False)\n",
    "optimal_model.load_weights(filepath='/kaggle/working/models/weights.02-0.48.hdf5')\n",
    "optimal_model.compile(Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "test_gen = ImageDataGenerator(rescale=1/255.)\n",
    "test_gen_flow = test_gen.flow_from_directory(directory=directory, target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "                                             class_mode='binary', batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "test_hist_new = optimal_model.evaluate(test_gen_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e353ac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2-3. ``flow_from_dataframe()``\n",
    "- label 값들은 수치형 인코딩된 상태가 아닌 문자열의 클래스 상태이어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208652d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T04:00:21.907448Z",
     "iopub.status.busy": "2021-10-04T04:00:21.907054Z",
     "iopub.status.idle": "2021-10-04T04:00:30.532057Z",
     "shell.execute_reply": "2021-10-04T04:00:30.53107Z",
     "shell.execute_reply.started": "2021-10-04T04:00:21.907316Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "paths = []\n",
    "types = []\n",
    "labels = []\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/cat-and-dog'):\n",
    "    for filename in filenames:\n",
    "        if '.jpg' in filename:\n",
    "            path = os.path.join(dirname, filename)\n",
    "            paths.append(path)\n",
    "            if '/training_set/' in path:\n",
    "                types.append('train')\n",
    "            elif '/test_set/' in path:\n",
    "                types.append('test')\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if 'dogs' in path:\n",
    "                labels.append('dog')\n",
    "            elif 'cats' in path:\n",
    "                labels.append('cat')\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "data_df = pd.DataFrame({'path': paths, 'type': types, 'label': labels})\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14727575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T04:00:50.399133Z",
     "iopub.status.busy": "2021-10-04T04:00:50.398153Z",
     "iopub.status.idle": "2021-10-04T04:00:50.434919Z",
     "shell.execute_reply": "2021-10-04T04:00:50.433845Z",
     "shell.execute_reply.started": "2021-10-04T04:00:50.399098Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# label 값을 LabelEncoding 하기\n",
    "data_df['label_enc'] = pd.factorize(data_df['label'])[0]\n",
    "# Train, Test 분할\n",
    "train_df = data_df[data_df['type'] == 'train']\n",
    "test_df = data_df[data_df['type'] == 'test']\n",
    "# Train, Valid 분할\n",
    "tr_df, val_df = train_test_split(train_df, stratify=train_df['label'], test_size=0.15, random_state=42)\n",
    "print('Train:', tr_df.shape)\n",
    "print('Valid:', val_df.shape)\n",
    "print('Test:', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a571b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T05:38:40.947815Z",
     "iopub.status.busy": "2021-10-03T05:38:40.947545Z",
     "iopub.status.idle": "2021-10-03T05:40:25.088409Z",
     "shell.execute_reply": "2021-10-03T05:40:25.087736Z",
     "shell.execute_reply.started": "2021-10-03T05:38:40.947786Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 데이터 유형에 맞게 ImageDataGenerator 정의\n",
    "tr_gen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rescale=1/255.)\n",
    "val_gen = ImageDataGenerator(rescale=1/255.)\n",
    "test_gen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "# Numpy Array Iterator\n",
    "tr_gen_flow = tr_gen.flow_from_dataframe(dataframe=tr_df, x_col='path', y_col='label',\n",
    "                                        target_size=(INPUT_SIZE, INPUT_SIZE), class_mode='binary',\n",
    "                                        batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_gen_flow = val_gen.flow_from_dataframe(dataframe=val_df, x_col='path', y_col='label',\n",
    "                                          target_size=(INPUT_SIZE, INPUT_SIZE), class_modoe='binary',\n",
    "                                          batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_gen_flow = test_gen.flow_from_dataframe(dataframe=test_df, x_col='path', y_col='label',\n",
    "                                            target_size=(INPUT_SIZE, INPUT_SIZE), class_mode='binary',\n",
    "                                            batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 정의\n",
    "model = create_mobilenet(INPUT_SIZE, verbose=False)\n",
    "# 모델 컴파일\n",
    "model.compile(Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# callbacks\n",
    "es_call = EarlyStopping(monitor='loss', mode='min', patience=4, verbose=1)\n",
    "lr_call = ReduceLROnPlateau(monitor='loss', mode='min', patience=3, verbose=1)\n",
    "# 모델 학습\n",
    "train_hist = model.fit(tr_gen_flow, epochs=2, validation_data=val_gen_flow, callbacks=[es_call, lr_call])\n",
    "# 모델 평가\n",
    "test_hist = model.evaluate(test_gen_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3544867",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Tensorflow Sequence & Albumentation 결합\n",
    "- ``tensorflow.keras.utils.Sequenct`` 클래스를 상속받아서 제네레이터 커스텀해서 정의\n",
    "- ``__len__`` 매직 메서드 새로 정의 : 한 에포크 당 배치 사이즈가 도는 steps 계산하는 것으로 변경\n",
    "- ``__getitem__`` 매직 메서드(리스트 인덱싱 매직메서드) 새로 정의 : 정의한 배치 사이즈만큼 데이터 소스에서 이미지, 레이블 가져와 -> 이미지 가공 처리 -> 배치 사이즈만큼의 이미지, 레이블 반환\n",
    "- ``on_epoch_end``: 한 에포크 마다 데이터 셔플할지 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8233d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T04:59:15.362982Z",
     "iopub.status.busy": "2021-10-04T04:59:15.362649Z",
     "iopub.status.idle": "2021-10-04T04:59:15.37775Z",
     "shell.execute_reply": "2021-10-04T04:59:15.376743Z",
     "shell.execute_reply.started": "2021-10-04T04:59:15.362949Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import sklearn\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "INPUT_SIZE = 224\n",
    "\n",
    "class CustomDS(Sequence):\n",
    "    def __init__(self, images, labels, batch_size, shuffle=False, augmentor=None, pre_func=None):\n",
    "        \"\"\"\n",
    "        images: 이미지 파일이 존재하는 디렉토리 경로 or Numpy array 형태의 이미지 -> 지금은 디렉토리 경로라고 가정\n",
    "        labels: 수치형으로 변환된 레이블\n",
    "        augmentor: Albumentation과 같은 써드파티 이미지 증강 모듈\n",
    "        pre_func: 이미지 픽셀값 스케일링 함수\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augmentor = augmentor\n",
    "        self.pre_func = pre_func\n",
    "        \n",
    "        # Keras에서는 데이터 구간이 1 ~ 10이 있다면 순차적으로 batch_size 만큼 차례대로 학습하기 떄문에 최초에만 데이터 셔플함\n",
    "        if self.shuffle:\n",
    "            self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        steps = int(np.ceil(self.images.shape[0] / self.batch_size))\n",
    "        return steps\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        images_batch = self.images[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        if self.labels is not None:\n",
    "            labels_batch = self.labels[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        images_batch_after = np.zeros((images_batch.shape[0], INPUT_SIZE, INPUT_SIZE, 3))\n",
    "        for idx in range(images_batch.shape[0]):\n",
    "            single_img = cv2.cvtColor(cv2.imread(images_batch[idx]), cv2.COLOR_BGR2RGB)\n",
    "            single_img = cv2.resize(single_img, (INPUT_SIZE, INPUT_SIZE))\n",
    "            if self.augmentor is not None:\n",
    "                single_img = self.augmentor(image=single_img)['image']\n",
    "            if self.pre_func is not None:\n",
    "                single_img = self.pre_func(single_img)\n",
    "            images_batch_after[idx] = single_img\n",
    "        return images_batch_after, labels_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        print('#### 셔플 수행! ####')\n",
    "        self.images, self.labels = sklearn.utils.shuffle(self.images, self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811dbae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T04:59:16.559012Z",
     "iopub.status.busy": "2021-10-04T04:59:16.558236Z",
     "iopub.status.idle": "2021-10-04T05:02:50.507469Z",
     "shell.execute_reply": "2021-10-04T05:02:50.506261Z",
     "shell.execute_reply.started": "2021-10-04T04:59:16.558974Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Augmentor, Scaling Function\n",
    "import albumentations as A\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_pre_func\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "augmentor = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.OneOf([\n",
    "        A.CLAHE(p=0.5),\n",
    "        A.ShiftScaleRotate(p=0.5)\n",
    "    ])\n",
    "])\n",
    "\n",
    "# Custom Image Generator\n",
    "train_gen = CustomDS(images=tr_df['path'].values, labels=tr_df['label_enc'].values, batch_size=64,\n",
    "                    shuffle=True, augmentor=augmentor, pre_func=mobilenet_pre_func)\n",
    "val_gen = CustomDS(images=val_df['path'].values, labels=val_df['label_enc'].values, batch_size=64,\n",
    "                  shuffle=False, augmentor=None, pre_func=mobilenet_pre_func)\n",
    "test_gen = CustomDS(images=test_df['path'].values, labels=test_df['label_enc'].values, batch_size=64,\n",
    "                   shuffle=False, augmentor=None, pre_func=mobilenet_pre_func)\n",
    "\n",
    "# Model\n",
    "model = create_mobilenet(input_size=INPUT_SIZE, verbose=False)\n",
    "# compile\n",
    "model.compile(Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# callbacks\n",
    "mc_call = ModelCheckpoint(filepath='/kaggle/working/models/weights.{epoch:02d}-{val_loss:.02f}.hdf5',\n",
    "                         monitor='val_loss', mode='min', verbose=0, period=1)\n",
    "lr_call = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.2, patience=5, verbose=1)\n",
    "es_call = EarlyStopping(monitor='val_loss', mode='min', patience=7, verbose=1)\n",
    "# fit\n",
    "train_hist = model.fit(train_gen, epochs=3, validation_data=val_gen, callbacks=[mc_call, lr_call, es_call])\n",
    "# evaluate\n",
    "evalutation = model.evaluate(test_gen)\n",
    "# Predict\n",
    "# model.predict(image_array ~)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150af12d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb240e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612132f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## GoogleNet(Inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64979d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T04:02:39.42204Z",
     "iopub.status.busy": "2021-10-05T04:02:39.421447Z",
     "iopub.status.idle": "2021-10-05T04:02:39.431633Z",
     "shell.execute_reply": "2021-10-05T04:02:39.430857Z",
     "shell.execute_reply.started": "2021-10-05T04:02:39.422002Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inception Module 생성 함수\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Concatenate\n",
    "\n",
    "\n",
    "def inception_module(x, filters_1x1, filters_3x3_reduce, filters_3x3, filters_5x5_reduce, filters_5x5, filters_pool_proj, name=None):\n",
    "    \"\"\"\n",
    "    x: previous layer\n",
    "    filters_1x1 : x에 1x1 컨볼루션 적용할 때 만들 필터개수\n",
    "    filters_3x3_reduce : x에 3x3 컨볼루션 적용 전 1x1 컨볼루션 적용할 때 만들 필터 개수\n",
    "    filters_3x3 : filters_3x3_reduce 이후에 적용할 원래 3x3 컨볼루션 시 만들 필터 개수\n",
    "    filters_5x5_reduce : x에 5x5 컨볼루션 적용 전 1x1 컨볼루션 적용할 때 만들 필터 개수\n",
    "    filters_5x5 : filters_5x5_reduce 이후에 적용할 원래 5x5 컨볼루션 시 만들 필터 개수\n",
    "    filters_pool : x에 3x3 max pooling 취한 후에 1x1 컨볼루션 적용할 때 만들 필터 개수\n",
    "    name : 인셉션 모듈 id\n",
    "    \"\"\"\n",
    "    conv_1x1 = Conv2D(filters=filters_1x1, kernel_size=1, padding='same', activation='relu')(x)\n",
    "    \n",
    "    conv_3x3 = Conv2D(filters=filters_3x3_reduce, kernel_size=1, padding='same', activation='relu')(x)\n",
    "    conv_3x3 = Conv2D(filters=filters_3x3, kernel_size=3, padding='same', activation='relu')(conv_3x3)\n",
    "    \n",
    "    conv_5x5 = Conv2D(filters=filters_5x5_reduce, kernel_size=1, padding='same', activation='relu')(x)\n",
    "    conv_5x5 = Conv2D(filters=filters_5x5, kernel_size=5, padding='same', activation='relu')(conv_5x5)\n",
    "    \n",
    "    conv_pool = MaxPooling2D(pool_size=3, strides=1, padding='same')(x)\n",
    "    conv_pool = Conv2D(filters=filters_pool_proj, kernel_size=1, padding='same', activation='relu')(conv_pool)\n",
    "    # Concatenate -> 합칠 layer들을 리스트로 functional API 형태로 넣어주기\n",
    "    module_output = Concatenate(axis=-1, name=name)([conv_1x1, conv_3x3, conv_5x5, conv_pool])\n",
    "    return module_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687995dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T04:03:45.289778Z",
     "iopub.status.busy": "2021-10-05T04:03:45.2894Z",
     "iopub.status.idle": "2021-10-05T04:03:45.314774Z",
     "shell.execute_reply": "2021-10-05T04:03:45.313805Z",
     "shell.execute_reply.started": "2021-10-05T04:03:45.289742Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_googlenet(in_shape=(224, 224, 3), n_classes=10):\n",
    "    input_tensor = Input(shape=in_shape)\n",
    "    \n",
    "    x = Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7/2')(input_tensor)\n",
    "    x = MaxPooling2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3/2')(x)\n",
    "    x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_3x3/1')(x)\n",
    "    x = Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3/1')(x)\n",
    "    x = MaxPooling2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3/2')(x)\n",
    "    \n",
    "       # 첫번째 inception 모듈\n",
    "    x = inception_module(x, filters_1x1=64,\n",
    "                         filters_3x3_reduce=96,\n",
    "                         filters_3x3=128,\n",
    "                         filters_5x5_reduce=16,\n",
    "                         filters_5x5=32,\n",
    "                         filters_pool_proj=32,\n",
    "                         name='inception_3a')\n",
    "    # 두번째 inception 모듈\n",
    "    x = inception_module(x,\n",
    "                         filters_1x1=128,\n",
    "                         filters_3x3_reduce=128,\n",
    "                         filters_3x3=192,\n",
    "                         filters_5x5_reduce=32,\n",
    "                         filters_5x5=96,\n",
    "                         filters_pool_proj=64,\n",
    "                         name='inception_3b')\n",
    "\n",
    "    x = MaxPooling2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3/2')(x)\n",
    "    \n",
    "    # 세번째 inception 모듈\n",
    "    x = inception_module(x,\n",
    "                         filters_1x1=192,\n",
    "                         filters_3x3_reduce=96,\n",
    "                         filters_3x3=208,\n",
    "                         filters_5x5_reduce=16,\n",
    "                         filters_5x5=48,\n",
    "                         filters_pool_proj=64,\n",
    "                         name='inception_4a')\n",
    "    # 네번째 inception 모듈\n",
    "    x = inception_module(x,\n",
    "                         filters_1x1=160,\n",
    "                         filters_3x3_reduce=112,\n",
    "                         filters_3x3=224,\n",
    "                         filters_5x5_reduce=24,\n",
    "                         filters_5x5=64,\n",
    "                         filters_pool_proj=64,\n",
    "                         name='inception_4b')\n",
    "    \n",
    "    # 다섯번째 inception 모듈\n",
    "    x = inception_module(x,\n",
    "                         filters_1x1=128,\n",
    "                         filters_3x3_reduce=128,\n",
    "                         filters_3x3=256,\n",
    "                         filters_5x5_reduce=24,\n",
    "                         filters_5x5=64,\n",
    "                         filters_pool_proj=64,\n",
    "                         name='inception_4c')\n",
    "    # 여섯번째 inception 모듈\n",
    "    x = inception_module(x,\n",
    "                         filters_1x1=112,\n",
    "                         filters_3x3_reduce=144,\n",
    "                         filters_3x3=288,\n",
    "                         filters_5x5_reduce=32,\n",
    "                         filters_5x5=64,\n",
    "                         filters_pool_proj=64,\n",
    "                         name='inception_4d')\n",
    "    # 일곱번째 inception 모듈\n",
    "    x = inception_module(x,\n",
    "                         filters_1x1=256,\n",
    "                         filters_3x3_reduce=160,\n",
    "                         filters_3x3=320,\n",
    "                         filters_5x5_reduce=32,\n",
    "                         filters_5x5=128,\n",
    "                         filters_pool_proj=128,\n",
    "                         name='inception_4e')\n",
    "\n",
    "    x = MaxPooling2D((3, 3), padding='same', strides=(2, 2), name='max_pool_4_3x3/2')(x)\n",
    "    # 여덟번째 inception 모듈\n",
    "    x = inception_module(x,\n",
    "                         filters_1x1=256,\n",
    "                         filters_3x3_reduce=160,\n",
    "                         filters_3x3=320,\n",
    "                         filters_5x5_reduce=32,\n",
    "                         filters_5x5=128,\n",
    "                         filters_pool_proj=128,\n",
    "                         name='inception_5a')\n",
    "    # 아홉번째 inception 모듈\n",
    "    x = inception_module(x,\n",
    "                         filters_1x1=384,\n",
    "                         filters_3x3_reduce=192,\n",
    "                         filters_3x3=384,\n",
    "                         filters_5x5_reduce=48,\n",
    "                         filters_5x5=128,\n",
    "                         filters_pool_proj=128,\n",
    "                         name='inception_5b')\n",
    "    \n",
    "    # Classifier Layer\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(rate=0.4)(x)\n",
    "    x = Dense(units=128)(x)\n",
    "    output = Dense(units=n_classes)(x)\n",
    "    \n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a27ba93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T04:13:27.185983Z",
     "iopub.status.busy": "2021-10-05T04:13:27.185624Z",
     "iopub.status.idle": "2021-10-05T04:13:27.206386Z",
     "shell.execute_reply": "2021-10-05T04:13:27.203375Z",
     "shell.execute_reply.started": "2021-10-05T04:13:27.185947Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random as python_random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import cv2\n",
    "import sklearn\n",
    "\n",
    "def zero_one_scaler(image):\n",
    "    return image/255.0\n",
    "\n",
    "def get_preprocessed_ohe(images, labels, pre_func=None):\n",
    "    # preprocessing 함수가 입력되면 이를 이용하여 image array를 scaling 적용.\n",
    "    if pre_func is not None:\n",
    "        images = pre_func(images)\n",
    "    # OHE 적용    \n",
    "    oh_labels = to_categorical(labels)\n",
    "    return images, oh_labels\n",
    "\n",
    "# 학습/검증/테스트 데이터 세트에 전처리 및 OHE 적용한 뒤 반환 \n",
    "def get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021):\n",
    "    # 학습 및 테스트 데이터 세트를  0 ~ 1사이값 float32로 변경 및 OHE 적용. \n",
    "    train_images, train_oh_labels = get_preprocessed_ohe(train_images, train_labels)\n",
    "    test_images, test_oh_labels = get_preprocessed_ohe(test_images, test_labels)\n",
    "    \n",
    "    # 학습 데이터를 검증 데이터 세트로 다시 분리\n",
    "    tr_images, val_images, tr_oh_labels, val_oh_labels = train_test_split(train_images, train_oh_labels, test_size=valid_size, random_state=random_state)\n",
    "    \n",
    "    return (tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels)\n",
    "\n",
    "class CustomDS(Sequence):\n",
    "    def __init__(self, images, labels, batch_size, shuffle, augmentor=None, pre_func=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augmentor = augmentor\n",
    "        self.pre_func = pre_func\n",
    "        \n",
    "        if self.shuffle:\n",
    "            self.on_epoch_end()\n",
    "            \n",
    "    def __len__(self):\n",
    "        steps = int(np.ceil(self.images.shape[0] / self.batch_size))\n",
    "        return steps\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        images_batch = self.images[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        if self.labels is not None:\n",
    "            labels_batch = self.labels[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        images_batch_after = np.zeros((images_batch.shape[0], INPUT_SIZE, INPUT_SIZE, 3), dtype=np.float32)\n",
    "        for idx in range(images_batch.shape[0]):\n",
    "            single_img = images_batch[idx]\n",
    "            single_img = cv2.resize(single_img, (INPUT_SIZE, INPUT_SIZE))\n",
    "            if self.augmentor is not None:\n",
    "                single_img = augmentor(image=single_img)['image']\n",
    "            if self.pre_func is not None:\n",
    "                single_img = self.pre_func(single_img)\n",
    "            images_batch_after[idx] = single_img\n",
    "        return images_batch_after, labels_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.images, self.labels = sklearn.utils.shuffle(self.images, self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dbe788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T04:13:27.832469Z",
     "iopub.status.busy": "2021-10-05T04:13:27.832203Z",
     "iopub.status.idle": "2021-10-05T04:13:28.723837Z",
     "shell.execute_reply": "2021-10-05T04:13:28.723Z",
     "shell.execute_reply.started": "2021-10-05T04:13:27.832442Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CIFAR10 데이터 재 로딩 및 Scaling/OHE 전처리 적용하여 학습/검증/데이터 세트 생성. \n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n",
    "\n",
    "(tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels) = \\\n",
    "    get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.2, random_state=2021)\n",
    "print(tr_images.shape, tr_oh_labels.shape, val_images.shape, val_oh_labels.shape, test_images.shape, test_oh_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c358ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T04:13:30.758456Z",
     "iopub.status.busy": "2021-10-05T04:13:30.757856Z",
     "iopub.status.idle": "2021-10-05T04:19:14.28463Z",
     "shell.execute_reply": "2021-10-05T04:19:14.283775Z",
     "shell.execute_reply.started": "2021-10-05T04:13:30.75842Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "INPUT_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "tr_ds = CustomDS(tr_images, tr_oh_labels, batch_size=BATCH_SIZE, augmentor=None, shuffle=True, pre_func=inception_preprocess)\n",
    "val_ds = CustomDS(val_images, val_oh_labels, batch_size=BATCH_SIZE, augmentor=None, shuffle=False, pre_func=inception_preprocess)\n",
    "\n",
    "model = create_googlenet(in_shape=(INPUT_SIZE, INPUT_SIZE, 3), n_classes=10)\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "lr_call = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.2, patience=4, verbose=1)\n",
    "es_call = EarlyStopping(monitor='val_loss', mode='min', patience=6, verbose=1)\n",
    "\n",
    "history = model.fit(tr_ds, epochs=10, validation_data=val_ds, callbacks=[lr_call, es_call])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5449f7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## ResNet\n",
    "#### 1. Identity Block 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f11952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-07T04:03:24.156947Z",
     "iopub.status.busy": "2021-10-07T04:03:24.15661Z",
     "iopub.status.idle": "2021-10-07T04:03:24.168112Z",
     "shell.execute_reply": "2021-10-07T04:03:24.167136Z",
     "shell.execute_reply.started": "2021-10-07T04:03:24.156918Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import add, Add\n",
    "\n",
    "# ResNet의 Identity Block(=Residual Block) 생성하는 함수\n",
    "def identity_block(input_tensor, middle_kernel_size, filters: list, stage, block):\n",
    "    \"\"\" Identity Block 생성\n",
    "    input_tensor: 입력 레이어 또는 previous 레이어\n",
    "    middle_kernel_size: Block 내 적용할 컨볼루션 중앙값(3 by 3 or 5 by 5)\n",
    "    filters: Block 내 적용할 3번의 컨볼루션 시 out시킬 필터개수. 리스트 형태\n",
    "    \"\"\"\n",
    "    filter1, filter2, filter3 = filters # filter1,2는 압축할 필터개수, filter3는 input_tensor와 같은 필터수\n",
    "    conv_name = f'res{stage}{block}_branch' # 명시할 컨볼루션 레이어 이름\n",
    "    bn_name = f'BN{stage}{block}_branch'    # 명시할 BatchNormalization 레이어 이름\n",
    "    \n",
    "    x = Conv2D(filters=filter1, kernel_size=(1, 1), kernel_initializer='he_normal', name=conv_name+'2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=-1, name=bn_name+'2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters=filter2, kernel_size=middle_kernel_size, padding='same', kernel_initializer='he_normal', name=conv_name+'2b')(x)\n",
    "    x = BatchNormalization(axis=-1, name=bn_name+'2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters=filter3, kernel_size=(1, 1), kernel_initializer='he_normal', name=conv_name+'2c')(x)\n",
    "    x = BatchNormalization(axis=-1, name=bn_name+'2c')(x)\n",
    "    \n",
    "    # Add\n",
    "    x = Add()([input_tensor, x])\n",
    "    # x = add([input_tensor, x]) 도 가능`\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5bcea2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 2. 각 Stage 내의 첫 번째 Identity Block에서 이전 Layer의 사이즈를 절반으로 줄이는 역할의 Block 생성\n",
    "- 위 함수랑 거의 동일하되 첫번째 컨볼루션 때 stride를 적용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66808618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-07T04:15:41.458616Z",
     "iopub.status.busy": "2021-10-07T04:15:41.458306Z",
     "iopub.status.idle": "2021-10-07T04:15:41.4722Z",
     "shell.execute_reply": "2021-10-07T04:15:41.471045Z",
     "shell.execute_reply.started": "2021-10-07T04:15:41.458585Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, middle_kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    filter1, filter2, filter3 = filters\n",
    "    conv_name = f'res{stage}{block}_branch'\n",
    "    bn_name = f'bn{stage}{block}_branch'\n",
    "    \n",
    "    # 1.첫 번째에 이전의 피처맵 사이즈를 줄이기 위해 컨볼루션에 strides 적용\n",
    "    x = Conv2D(filters=filter1, kernel_size=(1, 1), strides=strides, kernel_initializer='he_normal', name=conv_name+'2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=-1, name=bn_name+'2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters=filter2, kernel_size=middle_kernel_size, padding='same', kernel_initializer='he_normal', name=conv_name+'2b')(x)\n",
    "    x = BatchNormalization(axis=-1, name=bn_name+'2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters=filter3, kernel_size=(1, 1), kernel_initializer='he_normal', name=conv_name+'2c')(x)\n",
    "    x = BatchNormalization(axis=-1, name=bn_name+'2c')(x)\n",
    "    \n",
    "    # 이전 Layer 피처 맵 사이즈를 1번에서 줄였기 때문에 Skip Connection 할때도, 이전 Layer의 사이즈를 줄여서 넘겨주어야 함!\n",
    "    shortcut = Conv2D(filters=filter3, kernel_size=(1, 1), strides=strides, kernel_initializer='he_normal', name=conv_name+'shortcut_conv')(input_tensor)\n",
    "    shortcut = BatchNormalization(axis=-1, name=bn_name+'shortcut_bn')(shortcut)\n",
    "    \n",
    "    # Add\n",
    "    x = Add()([x, shortcut])\n",
    "    \n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7ec61",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 3. ResNet의 가장 intro 컨볼루션 레이어 생성\n",
    "- 입력 이미지의 Receptive Field를 넓게 해서 다양한 특징을 추출하기 위해 넓은 커널사이즈 컨볼루션 적용\n",
    "- 해당 예시에서는 원본 입력 이미지 사이즈는 (224, 224, 3)으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fdfc69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-07T04:24:30.026905Z",
     "iopub.status.busy": "2021-10-07T04:24:30.026556Z",
     "iopub.status.idle": "2021-10-07T04:24:30.04171Z",
     "shell.execute_reply": "2021-10-07T04:24:30.040725Z",
     "shell.execute_reply.started": "2021-10-07T04:24:30.026875Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import ZeroPadding2D, MaxPooling2D\n",
    "\n",
    "def do_first_conv(input_tensor):\n",
    "    # 7 by 7 컨볼루션 & strides=2 이후 224의 절반 112사이즈를 맞춰주기 위해서 제로패딩 추가\n",
    "    x = ZeroPadding2D(padding=(3, 3), name='conv1_pad')(input_tensor)\n",
    "    x = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='valid', kernel_initializer='he_normal', name='conv1')(x)\n",
    "    x = BatchNormalization(axis=-1, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # 3 by 3 맥스풀링 & Stride=2 적용하기 위해 제로패딩 또 추가\n",
    "    x = ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9132dcb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### ResNet 생성 함수(위 만든 함수들 모두 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004c9d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-07T04:35:27.290578Z",
     "iopub.status.busy": "2021-10-07T04:35:27.290278Z",
     "iopub.status.idle": "2021-10-07T04:35:27.310517Z",
     "shell.execute_reply": "2021-10-07T04:35:27.30968Z",
     "shell.execute_reply.started": "2021-10-07T04:35:27.29055Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "\n",
    "def create_resnet(in_shape=(224, 224, 3), n_classes=10):\n",
    "    input_tensor = Input(shape=in_shape)\n",
    "    # 초기 컨볼루션 적용\n",
    "    x = do_first_conv(input_tensor)\n",
    "    # 여러개의 Identity Block 생성\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1,1)) # strides=1로 설정해서 피처맵 사이즈 안줄임!\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "    \n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "    \n",
    "    # Classifier Layer\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(rate=0.3)(x)\n",
    "    x = Dense(units=200, activation='relu')(x)\n",
    "    x = Dropout(rate=0.3)(x)\n",
    "    output = Dense(units=n_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6efe7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-07T04:48:15.017077Z",
     "iopub.status.busy": "2021-10-07T04:48:15.01679Z",
     "iopub.status.idle": "2021-10-07T04:48:15.037299Z",
     "shell.execute_reply": "2021-10-07T04:48:15.035965Z",
     "shell.execute_reply.started": "2021-10-07T04:48:15.017045Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random as python_random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import cv2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "def zero_one_scaler(image):\n",
    "    return image/255.0\n",
    "\n",
    "def get_preprocessed_ohe(images, labels, pre_func=None):\n",
    "    # preprocessing 함수가 입력되면 이를 이용하여 image array를 scaling 적용.\n",
    "    if pre_func is not None:\n",
    "        images = pre_func(images)\n",
    "    # OHE 적용    \n",
    "    oh_labels = to_categorical(labels)\n",
    "    return images, oh_labels\n",
    "\n",
    "# 학습/검증/테스트 데이터 세트에 전처리 및 OHE 적용한 뒤 반환 \n",
    "def get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021):\n",
    "    # 학습 및 테스트 데이터 세트를  0 ~ 1사이값 float32로 변경 및 OHE 적용. \n",
    "    train_images, train_oh_labels = get_preprocessed_ohe(train_images, train_labels)\n",
    "    test_images, test_oh_labels = get_preprocessed_ohe(test_images, test_labels)\n",
    "    \n",
    "    # 학습 데이터를 검증 데이터 세트로 다시 분리\n",
    "    tr_images, val_images, tr_oh_labels, val_oh_labels = train_test_split(train_images, train_oh_labels, test_size=valid_size, random_state=random_state)\n",
    "    \n",
    "    return (tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels )\n",
    "\n",
    "\n",
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class CustomDS(Sequence):\n",
    "    def __init__(self, images, labels, batch_size, shuffle=False, augmentor=None, pre_func=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augmentor = augmentor\n",
    "        self.pre_func = pre_func\n",
    "        if self.shuffle:\n",
    "            self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        steps = int(np.ceil(self.images.shape[0] / self.batch_size))\n",
    "        return steps\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        images_batch = self.images[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        if self.labels is not None:\n",
    "            labels_batch = self.labels[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        images_batch_after = np.zeros((images_batch.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.float32)\n",
    "        for idx in range(images_batch.shape[0]):\n",
    "            single_img = images_batch[idx]\n",
    "            single_img = cv2.resize(single_img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            if self.augmentor is not None:\n",
    "                single_img = self.augmentor(image=single_img)['image']\n",
    "            if self.pre_func is not None:\n",
    "                single_img = self.pre_func(single_img)\n",
    "            images_batch_after[idx] = single_img\n",
    "        return images_batch_after, labels_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.images, self.labels = sklearn.utils.shuffle(self.images, self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14310442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-07T04:48:16.649821Z",
     "iopub.status.busy": "2021-10-07T04:48:16.649193Z",
     "iopub.status.idle": "2021-10-07T04:48:17.738952Z",
     "shell.execute_reply": "2021-10-07T04:48:17.737775Z",
     "shell.execute_reply.started": "2021-10-07T04:48:16.649771Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CIFAR10 데이터 재 로딩 및 OHE 전처리 적용하여 학습/검증/데이터 세트 생성. \n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n",
    "\n",
    "(tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels) = \\\n",
    "    get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.2, random_state=2021)\n",
    "print(tr_images.shape, tr_oh_labels.shape, val_images.shape, val_oh_labels.shape, test_images.shape, test_oh_labels.shape)\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
    "\n",
    "tr_ds = CustomDS(tr_images, tr_oh_labels, batch_size=BATCH_SIZE, augmentor=None, shuffle=True, pre_func=resnet_preprocess)\n",
    "val_ds = CustomDS(val_images, val_oh_labels, batch_size=BATCH_SIZE, augmentor=None, shuffle=False, pre_func=resnet_preprocess)\n",
    "test_ds = CustomDS(test_images, test_oh_labels, batch_size=BATCH_SIZE, augmentor=None, shuffle=False, pre_func=resnet_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb056ed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-07T04:48:17.741773Z",
     "iopub.status.busy": "2021-10-07T04:48:17.741427Z",
     "iopub.status.idle": "2021-10-07T04:51:41.767776Z",
     "shell.execute_reply": "2021-10-07T04:51:41.766765Z",
     "shell.execute_reply.started": "2021-10-07T04:48:17.741731Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "resnet_model = create_resnet(in_shape=(128, 128, 3), n_classes=10)\n",
    "resnet_model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "mc_call = ModelCheckpoint(filepath='/kaggle/working/weights.{epoch:02d}-{val_loss:.02f}.hdf5', monitor='val_loss', mode='min', verbose=0)\n",
    "lr_call = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.2, patience=5, verbose=1)\n",
    "es_call = EarlyStopping(monitor='val_loss', mode='min', patience=6, verbose=1)\n",
    "\n",
    "# fit\n",
    "history = resnet_model.fit(tr_ds, epochs=2, validation_data=val_ds, callbacks=[mc_call, lr_call, es_call])\n",
    "# evaluate\n",
    "test_hist = resnet_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c33f9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Fine Tuning\n",
    "- Cat and Dog 데이터로 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01db80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-09T05:21:30.207276Z",
     "iopub.status.busy": "2021-10-09T05:21:30.206623Z",
     "iopub.status.idle": "2021-10-09T05:21:33.297641Z",
     "shell.execute_reply": "2021-10-09T05:21:33.296904Z",
     "shell.execute_reply.started": "2021-10-09T05:21:30.207158Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def make_catndog_df():\n",
    "    paths = []\n",
    "    types = []\n",
    "    labels = []\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input/cat-and-dog'):\n",
    "        for filename in filenames:\n",
    "            if '.jpg' in filename:\n",
    "                path = os.path.join(dirname, filename)\n",
    "                paths.append(path)\n",
    "                if '/training_set/' in path:\n",
    "                    types.append('train')\n",
    "                elif '/test_set/' in path:\n",
    "                    types.append('test')\n",
    "                else:\n",
    "                    pass\n",
    "                if 'dogs' in path:\n",
    "                    labels.append('dog')\n",
    "                elif 'cats' in path:\n",
    "                    labels.append('cat')\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "    data_df = pd.DataFrame({'path':paths, 'type': types, 'label': labels})\n",
    "    print(data_df.shape)\n",
    "    return data_df\n",
    "\n",
    "data_df = make_catndog_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e175b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-09T05:29:57.217098Z",
     "iopub.status.busy": "2021-10-09T05:29:57.216648Z",
     "iopub.status.idle": "2021-10-09T05:29:57.231934Z",
     "shell.execute_reply": "2021-10-09T05:29:57.229657Z",
     "shell.execute_reply.started": "2021-10-09T05:29:57.217059Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom Datset 만들기\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import cv2\n",
    "import sklearn\n",
    "\n",
    "IMAGE_SIZE = 160\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class CustomDS(Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size, shuffle=False, augmentor=None, pre_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augmentor = augmentor\n",
    "        self.pre_func = pre_func\n",
    "        \n",
    "        if self.shuffle:\n",
    "            self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        steps = int(np.ceil(self.image_paths.shape[0] / self.batch_size))\n",
    "        return steps\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_batch = self.image_paths[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        if self.labels is not None:\n",
    "            label_batch = self.labels[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        image_batch_after = np.zeros((image_batch.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.float32)\n",
    "        for idx in range(image_batch.shape[0]):\n",
    "            single_img = cv2.cvtColor(cv2.imread(image_batch[idx]), cv2.COLOR_BGR2RGB)\n",
    "            single_img = cv2.resize(single_img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            if self.augmentor is not None:\n",
    "                single_img = augmentor(image=single_img)['image']\n",
    "            if self.pre_func is not None:\n",
    "                single_img = self.pre_func(single_img)\n",
    "            image_batch_after[idx] = single_img\n",
    "        \n",
    "        return image_batch_after, label_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.image_paths, self.labels = sklearn.utils.shuffle(self.image_paths, self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6471c5e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-09T05:33:05.862616Z",
     "iopub.status.busy": "2021-10-09T05:33:05.86233Z",
     "iopub.status.idle": "2021-10-09T05:33:05.871634Z",
     "shell.execute_reply": "2021-10-09T05:33:05.869282Z",
     "shell.execute_reply.started": "2021-10-09T05:33:05.862584Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 학습 데이터의 50%를 검증 데이터에 할당. \n",
    "def get_train_valid_test(data_df):\n",
    "    # 학습 데이터와 테스트 데이터용 Dataframe 생성. \n",
    "    train_df = data_df[data_df['type']=='train']\n",
    "    test_df = data_df[data_df['type']=='test']\n",
    "\n",
    "    # 학습 데이터의 image path와 label을 Numpy array로 변환 및 Label encoding\n",
    "    train_path = train_df['path'].values\n",
    "    train_label = pd.factorize(train_df['label'])[0]\n",
    "    \n",
    "    test_path = test_df['path'].values\n",
    "    test_label = pd.factorize(test_df['label'])[0]\n",
    "\n",
    "    tr_path, val_path, tr_label, val_label = train_test_split(train_path, train_label, test_size=0.5, random_state=2021)\n",
    "    print('학습용 path shape:', tr_path.shape, '검증용 path shape:', val_path.shape, \n",
    "      '학습용 label shape:', tr_label.shape, '검증용 label shape:', val_label.shape)\n",
    "    return tr_path, tr_label, val_path, val_label, test_path, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612700b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-09T05:30:00.440002Z",
     "iopub.status.busy": "2021-10-09T05:30:00.439743Z",
     "iopub.status.idle": "2021-10-09T05:30:00.450514Z",
     "shell.execute_reply": "2021-10-09T05:30:00.449763Z",
     "shell.execute_reply.started": "2021-10-09T05:30:00.439971Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam , RMSprop \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.applications import Xception, MobileNetV2\n",
    "\n",
    "def create_model(model_name='mobilenet', verbose=False):\n",
    "    \n",
    "    input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    if model_name == 'vgg16':\n",
    "        base_model = VGG16(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "    elif model_name == 'resnet50':\n",
    "        base_model = ResNet50V2(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "    elif model_name == 'xception':\n",
    "        base_model = Xception(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "    elif model_name == 'mobilenet':\n",
    "        base_model = MobileNetV2(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "    \n",
    "    bm_output = base_model.output\n",
    "\n",
    "    x = GlobalAveragePooling2D()(bm_output)\n",
    "    if model_name != 'vgg16':\n",
    "        x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(50, activation='relu', name='fc1')(x)\n",
    "    # 최종 output 출력을 softmax에서 sigmoid로 변환. \n",
    "    output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8b80c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-09T05:30:48.38028Z",
     "iopub.status.busy": "2021-10-09T05:30:48.379424Z",
     "iopub.status.idle": "2021-10-09T05:30:49.546891Z",
     "shell.execute_reply": "2021-10-09T05:30:49.545273Z",
     "shell.execute_reply.started": "2021-10-09T05:30:48.380242Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine Tuning 하기 위해서 Pretrained Model 레이어 구조 보기\n",
    "model = create_model('mobilenet', verbose=False)\n",
    "\n",
    "# model에 Layer 속성값이 리스트로 들어있음\n",
    "print(type(model.layers), model.layers[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b7ce4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-09T05:31:46.335277Z",
     "iopub.status.busy": "2021-10-09T05:31:46.334969Z",
     "iopub.status.idle": "2021-10-09T05:31:46.350739Z",
     "shell.execute_reply": "2021-10-09T05:31:46.34987Z",
     "shell.execute_reply.started": "2021-10-09T05:31:46.33524Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 레이어 이름과 각 레이어의 Trainable 상태 볼 수 있음 -> 이것을 활용해서 Fine Tuning 가능함\n",
    "for layer in model.layers[0:10]:\n",
    "    print('Name:', layer.name, 'Trainable:', layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fee36f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-09T05:45:02.23107Z",
     "iopub.status.busy": "2021-10-09T05:45:02.230511Z",
     "iopub.status.idle": "2021-10-09T05:56:23.962753Z",
     "shell.execute_reply": "2021-10-09T05:56:23.962006Z",
     "shell.execute_reply.started": "2021-10-09T05:45:02.231028Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_pre_func\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "FIRST_EPOCHS = 10\n",
    "SECOND_EPOCHS = 10\n",
    "\n",
    "tr_path, tr_label, val_path, val_label, test_path, test_label = get_train_valid_test(data_df)\n",
    "\n",
    "tr_ds = CustomDS(tr_path, tr_label, BATCH_SIZE, shuffle=True, augmentor=None, pre_func=mobilenet_pre_func)\n",
    "val_ds = CustomDS(val_path, val_label, BATCH_SIZE, shuffle=False, augmentor=None, pre_func=mobilenet_pre_func)\n",
    "\n",
    "# Pretrained Model 로드\n",
    "model = create_model(model_name='mobilenet', verbose=False)\n",
    "model.compile(Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Feature Extractor Layer들은 파라미터 Freeze 시키기\n",
    "for layer in model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "# Freeze 시킨 상태에서 10번 학습(즉, Custom으로 만든 Classifier Layer들만 파라미터 학습)\n",
    "first_hist = model.fit(tr_ds, epochs=FIRST_EPOCHS, validation_data=val_ds)\n",
    "\n",
    "# 모든 Layer들 다 Freeze 풀고 파라미터 학습(단, BN 레이어는 여전히 Freeze 시키기. Keras에선 그렇게 하는 것을 권고한다고 함..)\n",
    "for layer in model.layers:\n",
    "    if not isinstance(layer, layers.BatchNormalization):  # layer객체가 tf.keras.layers.BatchNormalization 클래스인지 확인\n",
    "        layer.trainable = True\n",
    "\n",
    "# 다시 학습시키기 위해 모델 재 컴파일 -> 학습률 약간 낮추기\n",
    "model.compile(Adam(lr=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "second_hist = model.fit(tr_ds, epochs=SECOND_EPOCHS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c92791",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Learning Rate Scheduler\n",
    "- Epoch Step에 따라 특정한 규칙을 적용해서 학습률을 조정\n",
    "- ``tf.keras.callbacks.LearningRateScheduler(scheduler_func, verbose=1)`` 형태로 사용자가 ``scheduler_func``의 Customized 함수를 만들어 넣으면 됨\n",
    "- ``scheduler_func``은 인자를 ``epoch``와 ``lr``을 입력받아서 학습률을 내맘대로 조정히고 조정된 학습률을 return 하도록 작성\n",
    "    - ``epoch`` 인자만을 입력받을 수 있긴한데, 그러려면 ``scheduler_func`` 함수 내부에서 최초의 lr를 정의해주어야 함!\n",
    "    - ``epoch``의 인덱스는 0부터 시작하기 때문에, 최초의 epoch는 ``epoch = 0`` 임. 그러므로 함수 내부에서 작성해줄 때 유의\n",
    "    - <a href='https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler'>관련 문서</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8566c13",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-10-10T04:48:36.797903Z",
     "iopub.status.busy": "2021-10-10T04:48:36.79746Z",
     "iopub.status.idle": "2021-10-10T04:48:37.321629Z",
     "shell.execute_reply": "2021-10-10T04:48:37.320807Z",
     "shell.execute_reply.started": "2021-10-10T04:48:36.797863Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exponentially decay\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "def scheduler_func(epoch, lr):\n",
    "    if epoch < 1:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * np.exp(-1.0)\n",
    "\n",
    "model = Sequential([Dense(20), Dense(5)])\n",
    "model.compile(SGD(), loss='mse')\n",
    "print('최초 SGD lr:', round(model.optimizer.lr.numpy(), 5))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler_func, verbose=1)\n",
    "history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n",
    "                   epochs=15, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7f370",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-10-10T04:52:08.132035Z",
     "iopub.status.busy": "2021-10-10T04:52:08.131321Z",
     "iopub.status.idle": "2021-10-10T04:52:08.542738Z",
     "shell.execute_reply": "2021-10-10T04:52:08.541892Z",
     "shell.execute_reply.started": "2021-10-10T04:52:08.131995Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step decay\n",
    "def step_decay(epoch):\n",
    "    init_lr = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 5.0\n",
    "    lr = init_lr * (drop ** np.floor(epoch/epochs_drop))\n",
    "    print('epoch:', epoch, 'lr:', lr)\n",
    "    return lr\n",
    "\n",
    "model = Sequential([Dense(20), Dense(5)])\n",
    "model.compile(SGD(), loss='mse')\n",
    "print('최초 lr:', round(model.optimizer.lr.numpy(), 5))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(step_decay, verbose=1)\n",
    "history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=20, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44582341",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-10-12T03:20:27.331300Z",
     "iopub.status.busy": "2021-10-12T03:20:27.330950Z",
     "iopub.status.idle": "2021-10-12T03:20:27.996899Z",
     "shell.execute_reply": "2021-10-12T03:20:27.996246Z",
     "shell.execute_reply.started": "2021-10-12T03:20:27.331260Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ramp Up and Step Down Decay\n",
    "def lrfn(epoch):\n",
    "    LR_START = 1e-5\n",
    "    LR_MAX = 1e-2\n",
    "    LR_RAMPUP_EPOCHS = 3\n",
    "    LR_SUSTAIN_EPOCHS = 3\n",
    "    LR_STEP_DECAY = 0.75\n",
    "    \n",
    "    def calc_fn(epoch):\n",
    "        if epoch < LR_RAMPUP_EPOCHS:\n",
    "            lr = ((LR_MAX - LR_START) / LR_RAMPUP_EPOCHS) * epoch + LR_START\n",
    "        elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "            lr = LR_MAX\n",
    "        else:\n",
    "            lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//2)\n",
    "        \n",
    "        print('epoch:', epoch, 'lr:', lr)\n",
    "        \n",
    "        return lr\n",
    "    \n",
    "    # 반드시 내포 함수인 calc_fn(epoch)를 호출해야함. \n",
    "    return calc_fn(epoch)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
    "model.compile(tf.keras.optimizers.SGD(), loss='mse')\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lrfn)\n",
    "history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n",
    "                    epochs=30, callbacks=[lr_scheduler], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fcbc25",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### ``tf.keras.experimental``로 Learning Rate Scheduler 하기\n",
    "- 단, ``tf.keras.experimental``는 ``tf.keras.optimizers.schedule.LearningRateSchedule``을 상속받기 때문에 모델 comile시 optimizer 설정에 넣어주어야 함!\n",
    "- optimizer 설정에 넣어주기 때문에 해당 Learning Rate Scheduler 방법은 학습 시 배치 단위로 학습률을 조정함\n",
    "    - ``tf.keras.callbacks.LearningRateScheduler``는 epoch 단위로 학습률을 조정했음!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b91fe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T03:07:04.940144Z",
     "iopub.status.busy": "2021-10-12T03:07:04.939328Z",
     "iopub.status.idle": "2021-10-12T03:07:05.138967Z",
     "shell.execute_reply": "2021-10-12T03:07:05.138316Z",
     "shell.execute_reply.started": "2021-10-12T03:07:04.940107Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.experimental import CosineDecay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_scheduler(epochs_lst, lr_lst, title=None):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(epochs_lst, lr_lst)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "cos_decay = CosineDecay(initial_learning_rate=0.01, decay_steps=30, alpha=0.0, name='test')  # name 속성 넣으면 에러 발생\n",
    "\n",
    "steps_lst = range(0, 30)\n",
    "lr_lst = cos_decay(steps_lst)\n",
    "\n",
    "plot_scheduler(steps_lst, lr_lst, title='Cosine Decay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702c260",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-10-12T03:11:49.510183Z",
     "iopub.status.busy": "2021-10-12T03:11:49.509254Z",
     "iopub.status.idle": "2021-10-12T03:11:51.151802Z",
     "shell.execute_reply": "2021-10-12T03:11:51.150928Z",
     "shell.execute_reply.started": "2021-10-12T03:11:49.510135Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델에 넣어보기\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.experimental import CosineDecay\n",
    "import numpy as np\n",
    "\n",
    "cos_decay = CosineDecay(initial_learning_rate=0.1, decay_steps=30, alpha=0.0, name='Cosine_Decay')\n",
    "\n",
    "model = Sequential([Dense(10), Dense(5)])\n",
    "model.compile(Adam(learning_rate=cos_decay), loss='mse')\n",
    "\n",
    "model.fit(np.arange(50).reshape(5, -1), np.zeros((5)), epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89b73f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T03:17:21.309515Z",
     "iopub.status.busy": "2021-10-12T03:17:21.309252Z",
     "iopub.status.idle": "2021-10-12T03:17:21.699964Z",
     "shell.execute_reply": "2021-10-12T03:17:21.699292Z",
     "shell.execute_reply.started": "2021-10-12T03:17:21.309487Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cosine Decay Restart\n",
    "from tensorflow.keras.experimental import CosineDecayRestarts\n",
    "\n",
    "# t_mul은 first_decay_steps를 할 때마다 몇 배 단위로 할 것인지\n",
    "# m_mul은 warm restart시 마다 적용될 학습률을 그 전 학습률에 비해 얼마나 늘리거나 줄일지\n",
    "cos_decay_restart = CosineDecayRestarts(initial_learning_rate=0.1, first_decay_steps=10, t_mul=2, m_mul=0.8, alpha=0.0)\n",
    "\n",
    "steps_lst = [x for x in range(100)]\n",
    "lr_lst = cos_decay_restart(steps_lst)\n",
    "\n",
    "plot_scheduler(steps_lst, lr_lst, title='Cosine_Decay_Restart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c317bb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 64.58968,
   "end_time": "2021-10-12T03:49:51.992058",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-12T03:48:47.402378",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
