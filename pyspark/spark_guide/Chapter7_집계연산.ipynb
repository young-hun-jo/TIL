{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "national-romance",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#집계-연산\" data-toc-modified-id=\"집계-연산-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>집계 연산</a></span><ul class=\"toc-item\"><li><span><a href=\"#집계-함수\" data-toc-modified-id=\"집계-함수-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>집계 함수</a></span><ul class=\"toc-item\"><li><span><a href=\"#count\" data-toc-modified-id=\"count-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>count</a></span></li><li><span><a href=\"#countDistinct\" data-toc-modified-id=\"countDistinct-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>countDistinct</a></span></li><li><span><a href=\"#approx_count_distinct\" data-toc-modified-id=\"approx_count_distinct-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>approx_count_distinct</a></span></li><li><span><a href=\"#first,-last\" data-toc-modified-id=\"first,-last-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>first, last</a></span></li><li><span><a href=\"#min,-max\" data-toc-modified-id=\"min,-max-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>min, max</a></span></li><li><span><a href=\"#sum\" data-toc-modified-id=\"sum-1.1.6\"><span class=\"toc-item-num\">1.1.6&nbsp;&nbsp;</span>sum</a></span></li><li><span><a href=\"#sumDistinct\" data-toc-modified-id=\"sumDistinct-1.1.7\"><span class=\"toc-item-num\">1.1.7&nbsp;&nbsp;</span>sumDistinct</a></span></li><li><span><a href=\"#avg,-mean\" data-toc-modified-id=\"avg,-mean-1.1.8\"><span class=\"toc-item-num\">1.1.8&nbsp;&nbsp;</span>avg, mean</a></span></li><li><span><a href=\"#분산과-표준편차\" data-toc-modified-id=\"분산과-표준편차-1.1.9\"><span class=\"toc-item-num\">1.1.9&nbsp;&nbsp;</span>분산과 표준편차</a></span></li><li><span><a href=\"#비대칭도(skewness),-첨도(kurtosis)\" data-toc-modified-id=\"비대칭도(skewness),-첨도(kurtosis)-1.1.10\"><span class=\"toc-item-num\">1.1.10&nbsp;&nbsp;</span>비대칭도(skewness), 첨도(kurtosis)</a></span></li><li><span><a href=\"#공분산과-상관관계\" data-toc-modified-id=\"공분산과-상관관계-1.1.11\"><span class=\"toc-item-num\">1.1.11&nbsp;&nbsp;</span>공분산과 상관관계</a></span></li><li><span><a href=\"#복합-데이터-타입의-집계\" data-toc-modified-id=\"복합-데이터-타입의-집계-1.1.12\"><span class=\"toc-item-num\">1.1.12&nbsp;&nbsp;</span>복합 데이터 타입의 집계</a></span></li></ul></li><li><span><a href=\"#그룹화\" data-toc-modified-id=\"그룹화-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>그룹화</a></span><ul class=\"toc-item\"><li><span><a href=\"#표현식을-이용한-그룹화\" data-toc-modified-id=\"표현식을-이용한-그룹화-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>표현식을 이용한 그룹화</a></span></li></ul></li><li><span><a href=\"#윈도우-함수\" data-toc-modified-id=\"윈도우-함수-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>윈도우 함수</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-cutting",
   "metadata": {},
   "source": [
    "# 집계 연산\n",
    "- 집계 시 인지해야 할 중요한 점은\n",
    "    - 집계한다는 것은 빅데이터에서 정확한 답을 얻기 위함임. 그런데 어느정도 즉, 수용가능한 정도의 정확도에 맞춰 근사치를 계산하는 것이 연산, 네트워크, 저장소 등의 비용을 고려하는 것임\n",
    "    - 따라서 **근사치 계산용 함수**를 사용해 스파크 잡의 실행과 속도를 개선할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statewide-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "          .option('header', 'true')\\\n",
    "          .option('inferSchema', 'true')\\\n",
    "          .load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/retail-data/all/*.csv')\\\n",
    "          .coalesce(5)  # 컬레스를 사용해 리파티셔닝(파티션 감소)\n",
    "\n",
    "# 해당 데이터프레임을 메모리에 캐싱\n",
    "df.cache()\n",
    "# 트랜스포메이션 결과(액션) 관찰하기 위해 임시 테이블로 생성\n",
    "df.createOrReplaceTempView('dfTable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-static",
   "metadata": {},
   "source": [
    "- ``count()``메소드는 즉시 수행하는 액션 메소드임! \n",
    "    - 데이터프레임의 행 개수를 집계하는 데 사용되기도 하지만 메모리에 데이터프레임 캐싱작업을 수행하는 용도로도 사용됨!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "disturbed-zoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541909\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "count = df.count()\n",
    "print(count)\n",
    "print(count == 541909)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-manhattan",
   "metadata": {},
   "source": [
    "## 집계 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-insider",
   "metadata": {},
   "source": [
    "### count\n",
    "- 해당 예제에서는 액션이 아닌 **트랜스포메이션(lazy execution)** 으로 동작함\n",
    "- 두 가지 방식으로 사용 가능\n",
    "    * ``count(\"column_name\")``\n",
    "        - 특정 칼럼명을 지정하면 ``null``값 제외하고 집계\n",
    "    * ``count(*)``\n",
    "        - 단, SQL과 마착나지로 ``*`` 사용해서 카운트 시, 만약 칼럼들에 ``null``값이 포함되어 있다면 ``null``이 있는 것도 모두 집계\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interim-cutting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "capital-breast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| count|\n",
      "+------+\n",
      "|541909|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count \n",
    "\n",
    "df.select(count('StockCode').alias('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alert-electron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(count('*')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-questionnaire",
   "metadata": {},
   "source": [
    "### countDistinct\n",
    "\n",
    "- unique한 row 개수를 출력하는데, 이 메소드는 주로 개별 컬럼에다가 적용하는것이 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "constant-insurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Customer_cnt|\n",
      "+------------+\n",
      "|        4372|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.select(countDistinct('CustomerID').alias('Customer_cnt')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-harbor",
   "metadata": {},
   "source": [
    "### approx_count_distinct\n",
    "- 대규모 데이터셋을 다루다 보면 정확한 unique한 개수는 필요없고 대략적인 근사치만 필요할 때가 있음\n",
    "- 추가 파라미터인 최대 추정 오류율(maximum estimation error)이 존재하는데, 0.1이면 오류율 10%을 의미\n",
    "    - 단, 39%의 오류율까지만 허용함. 그 이상 입력 시 에러 발생\n",
    "- ``countDistinct``보다 훨씬 속도가 빠르며, 데이터셋이 매우 커짐에 따라 해당 메서드의 유용성이 증가(단, 대략적인 근사치가 필요할 때만!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "professional-tennessee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|approx_Customer_cnt|\n",
      "+-------------------+\n",
      "|               4336|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df.select(approx_count_distinct('CustomerID', 0.1).alias('approx_Customer_cnt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "assured-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|approx_Customer_cnt|\n",
      "+-------------------+\n",
      "|               5955|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df.select(approx_count_distinct('CustomerID', 0.39).alias('approx_Customer_cnt')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-anthony",
   "metadata": {},
   "source": [
    "### first, last\n",
    "- 데이터프레임 특정 칼럼의 첫 번째 값 또는 마지막 값을 반환\n",
    "- 이 함수들은 Row-based으로 동작함\n",
    "    - Row-based/Column-based 설명 <a href='https://bi-insider.com/business-intelligence/column-and-row-based-database-storage/'>링크</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "naked-preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----------------------------+\n",
      "|first(Description)                |last(Description)            |\n",
      "+----------------------------------+-----------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|BAKING SET 9 PIECE RETROSPOT |\n",
      "+----------------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "df.select(first('Description'), last('Description')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-entrepreneur",
   "metadata": {},
   "source": [
    "### min, max\n",
    "- 데이터프레임 특정 칼럼의 최솟값, 최대값을 반환(문자열도 오름,내림차순으로 자동 정렬 후 추출됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "classical-argentina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------+--------------+\n",
      "|max(Country)|min(Country)|max(UnitPrice)|min(UnitPrice)|\n",
      "+------------+------------+--------------+--------------+\n",
      "| Unspecified|   Australia|       38970.0|     -11062.06|\n",
      "+------------+------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min \n",
    "\n",
    "df.select(max('Country'), min('Country'), max('UnitPrice'), min('UnitPrice')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-dynamics",
   "metadata": {},
   "source": [
    "### sum\n",
    "- 모든 값을 합산\n",
    "- 문자열을 sum 하면 null로 반환 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "civic-numbers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|  sum(UnitPrice)|\n",
      "+----------------+\n",
      "|2498803.97400038|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df.select(sum('UnitPrice')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spoken-platform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|sum(Country)|\n",
      "+------------+\n",
      "|        null|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 문자열을 sum 하면 null로 반환 됨\n",
    "df.select(sum('Country')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-officer",
   "metadata": {},
   "source": [
    "### sumDistinct\n",
    "- unique한 값들만 합산 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "induced-conservative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|sum(DISTINCT UnitPrice)|\n",
      "+-----------------------+\n",
      "|      611388.3910000001|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "\n",
    "df.select(sumDistinct('UnitPrice')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-copper",
   "metadata": {},
   "source": [
    "### avg, mean\n",
    "- ``sum / count``로 평균값을 직접 구할 수 있지만, avg, mean으로 대체 가능\n",
    "    - ``avg``는 ``pyspark.sql.functions`` 패키지에 있지만,\n",
    "    - ``mean``은 ``expr`` 메소드를 사용한 후 쿼리문 형태로 사용 가능함\n",
    "- 참고로 대부분의 집계 함수는 unique한 값들만 사용해 집계를 수행하는 방식을 지원함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "further-finnish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|    avg_quantity|   mean_quantity|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "df.select(count('Quantity').alias('total_transactions'),\n",
    "         sum('Quantity').alias('total_purchases'),\n",
    "         avg('Quantity').alias('avg_quantity'),\n",
    "         expr('mean(Quantity)').alias('mean_quantity')) \\\n",
    "  .selectExpr('total_purchases / total_transactions',\n",
    "             'avg_quantity',\n",
    "             'mean_quantity').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-laser",
   "metadata": {},
   "source": [
    "### 분산과 표준편차\n",
    "- 스파크는 표본분산(표준편차)와 모분산(표준편차)를 개별로 제공\n",
    "    - 위 2개 계산 방식은 다르기 떄문에 메서드도 다름\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "upper-council",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+---------------------+--------------------+\n",
      "|var_samp(Quantity)| var_pop(Quantity)|stddev_samp(Quantity)|stddev_pop(Quantity)|\n",
      "+------------------+------------------+---------------------+--------------------+\n",
      "|47559.391409298754|47559.303646609056|   218.08115785023418|  218.08095663447796|\n",
      "+------------------+------------------+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_samp, stddev_samp # sample = 표본\n",
    "from pyspark.sql.functions import var_pop, stddev_pop   # pop=population = 모집단\n",
    "\n",
    "df.select(var_samp('Quantity'), var_pop('Quantity'), stddev_samp('Quantity'), stddev_pop('Quantity')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-nation",
   "metadata": {},
   "source": [
    "### 비대칭도(skewness), 첨도(kurtosis)\n",
    "- 확률변수, 확률분포를 기반으로 데이터를 모델링할 때 해당 척도들을 알아보는 것이 중요할 때 있음\n",
    "- 값을 모두 정규화시킨 후, 왜도, 첨도를 계산했을 때,\n",
    "    - 왜도\n",
    "        - 양수: 왼쪽으로 기울어짐(오른쪽으로 꼬리가 긴)\n",
    "        - 음수: 오른쪽으로 기울어짐(왼쪽으로 꼬리가 긴)\n",
    "        - 0: 정규분포 모양과 유사\n",
    "    - 첨도\n",
    "        - 양수: 뾰족함(즉, 표준편차값이 작을 때 모양)\n",
    "        - 음수: 퍼져있음(즉, 표준편차값이 클 때 모양)\n",
    "        - 0: 정규분포 모양과 유사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "textile-rhythm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|skewness(UnitPrice)|kurtosis(UnitPrice)|\n",
      "+-------------------+-------------------+\n",
      "| 186.50645547025985| 59005.174662675585|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "df.select(skewness(\"UnitPrice\"), kurtosis(\"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-elevation",
   "metadata": {},
   "source": [
    "### 공분산과 상관관계\n",
    "- 공분산은 데이터 입력의 범위를 고려하지 않음. 그래서 ``공분산 --(normalize)--> 상관계수``\n",
    "- 공분산도 모공분산, 표본공분산으로 메소드가 나뉘므로 사용상황에 따라 잘 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "included-dublin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|corr(Quantity, UnitPrice)|covar_pop(Quantity, UnitPrice)|covar_samp(Quantity, UnitPrice)|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|     -0.00123492454487...|           -26.058713170967906|            -26.058761257936858|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\"), covar_pop(\"Quantity\", \"UnitPrice\"), covar_samp(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-amendment",
   "metadata": {},
   "source": [
    "### 복합 데이터 타입의 집계\n",
    "- 예를들어, 특정 칼럼의 값을 리스트, 셋(set)으로 수집이 가능한 메소드들\n",
    "- ``agg``메소드를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "heard-blade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+\n",
      "|collect_list(Country)|collect_set(Country)|\n",
      "+---------------------+--------------------+\n",
      "| [United Kingdom, ...|[Portugal, Italy,...|\n",
      "+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, collect_set\n",
    "\n",
    "df.agg(collect_list('Country'), collect_set('Country')).show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-italic",
   "metadata": {},
   "source": [
    "## 그룹화\n",
    "- 데이터 그룹 기반의 집계를 수행\n",
    "    - ex) 고유한 송장번호를 기준으로 그룹화 -> 그룹별 물품 수를 카운트\n",
    "    - pandas ex) ``df.groupby('InvoiceNo')['물품'].count()``, ``df.groupby('InvoiceNo').agg({'물품': 'count})``\n",
    "- 데이터 그룹화 집계 시에도 지연방식으로 수행됨\n",
    "    - 데이터 그룹화 시, ``RelationalGroupedDataset``이 반환되고,\n",
    "    - 집계 연산 수행 시, ``DataFrame``이 반환됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "relevant-chocolate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   536596|      null|    6|\n",
      "|   537252|      null|    1|\n",
      "|   538041|      null|    1|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 그룹화시킬 칼럼을 다수 입력 가능 -> count를 메소드로 사용하는 방식\n",
    "df.groupby(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-recording",
   "metadata": {},
   "source": [
    "### 표현식을 이용한 그룹화\n",
    "- ``count``는 메서드로도 사용이 가능하지만 import 해서 함수를 사용하는 것을 권고\n",
    "- ``select``구문에서 집계를 사용하는 것보다 ``agg`` 메서드를 사용하는 것이 한 번에 여러 집계를 처리할 수 있으며 집계에 표현식(``expr``)을 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "immediate-cleaners",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+\n",
      "|InvoiceNo|quan|quan2|\n",
      "+---------+----+-----+\n",
      "|   536596|   6|    6|\n",
      "|   536938|  14|   14|\n",
      "|   537252|   1|    1|\n",
      "|   537691|  20|   20|\n",
      "|   538041|   1|    1|\n",
      "|   538184|  26|   26|\n",
      "|   538517|  53|   53|\n",
      "|   538879|  19|   19|\n",
      "|   539275|   6|    6|\n",
      "|   539630|  12|   12|\n",
      "+---------+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count # import해서 count를 메소드로 사용 \n",
    "\n",
    "df.groupBy(\"InvoiceNo\") \\\n",
    "  .agg(count(\"Quantity\").alias('quan'),\n",
    "      expr(\"count(Quantity) as quan2\")\n",
    "      ) \\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-paste",
   "metadata": {},
   "source": [
    "## 윈도우 함수\n",
    "- **윈도우 명세(Window Specification)**를 정의해주어야 함\n",
    "- ``group-by``는 모든 row 레코드들이 단일 그룹으로만 이동\n",
    "- 반면, 윈도우 함수는 프레임에 입력되는 모든 로우에 대해 결괏값을 계산\n",
    "    - 프레임: row 그룹 기반의 테이블\n",
    "    - 각 로우는 하나 이상의 프레임에 할당될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "juvenile-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜를 기준으로 윈도우 함수를 만들려 하는 예제이므로 date 칼럼 생성\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "# 임시 view 테이블로 생성 for 트랜스포메이션 적용 결과 보기 위함\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-showcase",
   "metadata": {},
   "source": [
    "- SQL의 윈도우 함수: ``window_function() over (partition by 칼럼1, 칼럼2 order by 칼럼3 desc)``\n",
    "- Pyspark의 윈도우 함수\n",
    "    - ``partitionBy``: coalesce 메소드와 같이 파티셔닝관련 메소드와는 전혀 관련 없음! 그룹을 어떻게 나눌지 결정하는 개념임!\n",
    "    - ``orderBy``: ``partitionBy``의 정렬 방식을 의미\n",
    "    - ``rowsBetween``: 윈도우 함수의 프레임 명세를 의미하며 입력된 row들의 참조를 기반으로 프레임에 row가 포함될 수 있는지 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "unlike-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex1 -> \"고객, 시간대 별로 최대 구매 개수 집계\"\n",
    "# 첫 로우부터 현재로우까지 모두 프레임에 포함될 수 있는지 확인하는 명세서 예제임\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# 윈도우 명세서\n",
    "windowSpec = Window.partitionBy(\"CustomerId\", \"date\")\\\n",
    "                   .orderBy(desc(\"Quantity\"))\\\n",
    "                   .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)  # over에 윈도우명세서를 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "specified-jungle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "# 칼럼(표현식) 타입을 반환함 -> DataFrame의 select 구문에서 사용 가능 함!\n",
    "print(type(maxPurchaseQuantity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "heard-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex2 -> 고객별로 최대 구매 수량을 가진 날짜가 언제인지\n",
    "from pyspark.sql.functions import rank, dense_rank\n",
    "\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 위에서 수행한 3개의 윈도우 함수 집계 값을 select in dataframe 하기 -> 날짜 파싱에러 발생(일, 시간 한자리, 두자리 포맷 불일치.. 이거 통일 시킬 순 없나..?)\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\")\\\n",
    "          .orderBy(\"CustomerId\")\\\n",
    "          .select(col(\"CustomerId\"),\n",
    "                 col(\"date\"),\n",
    "                 col(\"Quantity\"),\n",
    "                 purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "                 purchaseRank.alias(\"quantityRank\"),\n",
    "                 maxPurchaseQuantity.alias(\"maxPurchaseQuantity\"))\\\n",
    "          .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-oregon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
