{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "native-executive",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#스파크-데이터-타입으로-변환\" data-toc-modified-id=\"스파크-데이터-타입으로-변환-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>스파크 데이터 타입으로 변환</a></span></li><li><span><a href=\"#Boolean-데이터-타입-다루기\" data-toc-modified-id=\"Boolean-데이터-타입-다루기-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Boolean 데이터 타입 다루기</a></span></li><li><span><a href=\"#수치형-데이터-타입-다루기\" data-toc-modified-id=\"수치형-데이터-타입-다루기-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>수치형 데이터 타입 다루기</a></span></li><li><span><a href=\"#문자열-데이터-타입-다루기\" data-toc-modified-id=\"문자열-데이터-타입-다루기-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>문자열 데이터 타입 다루기</a></span><ul class=\"toc-item\"><li><span><a href=\"#문자열---정규표현식\" data-toc-modified-id=\"문자열---정규표현식-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>문자열 - 정규표현식</a></span></li></ul></li><li><span><a href=\"#날짜와-타임스탬프-데이터-타입-다루기\" data-toc-modified-id=\"날짜와-타임스탬프-데이터-타입-다루기-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>날짜와 타임스탬프 데이터 타입 다루기</a></span></li><li><span><a href=\"#null-값--다루기\" data-toc-modified-id=\"null-값--다루기-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span><code>null</code> 값  다루기</a></span><ul class=\"toc-item\"><li><span><a href=\"#coalesce\" data-toc-modified-id=\"coalesce-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span><code>coalesce</code></a></span></li><li><span><a href=\"#ifnull,-nullif,-nvl,-nvl2\" data-toc-modified-id=\"ifnull,-nullif,-nvl,-nvl2-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span><code>ifnull, nullif, nvl, nvl2</code></a></span></li><li><span><a href=\"#drop\" data-toc-modified-id=\"drop-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span><code>drop</code></a></span></li><li><span><a href=\"#fill\" data-toc-modified-id=\"fill-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span><code>fill</code></a></span></li><li><span><a href=\"#replace\" data-toc-modified-id=\"replace-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span><code>replace</code></a></span></li></ul></li><li><span><a href=\"#복합-데이터-타입-다루기\" data-toc-modified-id=\"복합-데이터-타입-다루기-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>복합 데이터 타입 다루기</a></span><ul class=\"toc-item\"><li><span><a href=\"#구조체\" data-toc-modified-id=\"구조체-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>구조체</a></span></li><li><span><a href=\"#배열\" data-toc-modified-id=\"배열-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>배열</a></span></li><li><span><a href=\"#map\" data-toc-modified-id=\"map-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span><code>map</code></a></span></li><li><span><a href=\"#JSON-다루기\" data-toc-modified-id=\"JSON-다루기-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>JSON 다루기</a></span></li><li><span><a href=\"#사용자-정의-함수(UDF)\" data-toc-modified-id=\"사용자-정의-함수(UDF)-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>사용자 정의 함수(UDF)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regular-joining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mall\u001b[m\u001b[m    \u001b[1m\u001b[36mby-day\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/retail-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forward-closer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "     .option('header', 'true')\\\n",
    "     .option('inferSchema', 'true')\\\n",
    "     .load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/retail-data/by-day/2010-12-01.csv')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bizarre-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포메이션 결과를 확인하기 위해 임시테이블인 뷰로 등록\n",
    "df.createOrReplaceTempView('dfTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gentle-progressive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-pregnancy",
   "metadata": {},
   "source": [
    "## 스파크 데이터 타입으로 변환\n",
    "- 스파크 자체 데이터 타입으로 변환시키기 위해 ``lit`` 함수를 분명히 기억!!\n",
    "    * ``lit``은 다른 언어의 데이터 타입을 스파크 데이터 타입에 맞게 변환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efficient-divide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "|  5|five|5.0|\n",
      "+---+----+---+\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "+---+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "test = df.select(lit(5), lit('five'), lit(5.0))\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-friendship",
   "metadata": {},
   "source": [
    "## Boolean 데이터 타입 다루기\n",
    "- Boolean은 로우를 필터링할 때 매우 자주 사용됨\n",
    "- 주로 조건문이 일치하면 True, 일치하지 않으면 False로 반환시켜 필터링 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "convinced-container",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rational-terror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.where(col('InvoiceNo') != 536365)\\\n",
    "  .select(\"InvoiceNo\", \"Description\")\\\n",
    "  .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exciting-disco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col('InvoiceNo') != 536365)\\\n",
    "  .selectExpr(\"InvoiceNo\",\"Description\")\\\n",
    "  .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exposed-frequency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# 문자열 표현식으로도 사용 가능\n",
    "df.where(\"InvoiceNo != 536365\")\\\n",
    "  .select(expr(\"InvoiceNo\"), expr(\"Description\"))\\\n",
    "  .show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-disabled",
   "metadata": {},
   "source": [
    "- ``instr(string, substr)``: ``string`` 문자열에서 ``substr``이 시작하는 첫 번째 인덱스를 반환(단, **Pyspark에서 문자열 index 번호는 1부터 시작!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fancy-princeton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여러가지 필터링 조건\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "priceFilter = col('UnitPrice') > 600\n",
    "descriFilter = instr(df.Description, 'POSTAGE') >= 1\n",
    "\n",
    "# 논리연산자 사용가능\n",
    "\"\"\"\n",
    "and로 필터추가하려면 연속적으로 where 구문 붙여서 하나의 문장으로 만들면 됨\n",
    "or(|)로 필터추가하려면 동일한 구문 조건 안에 넣어야 함(하단 참조)\n",
    "\"\"\"\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descriFilter).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-cradle",
   "metadata": {},
   "source": [
    "- ``select``시, 불리언 타입의 칼럼을 추출할 때는 자동으로 True인 값들만 추출함(Boolean Indexing 처럼!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "frequent-container",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|UnitPrice|isExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 불리언 표현식을 사용해서 DF 필터링하기\n",
    "DOTCodeFilter = col('StockCode') == 'DOT'\n",
    "priceFilter = col('UnitPrice') >= 600\n",
    "descriFilter = instr(df.Description, 'POSTAGE') >= 1\n",
    "\n",
    "df.withColumn('isExpensive', DOTCodeFilter & (priceFilter | descriFilter))\\\n",
    "  .where('isExpensive')\\\n",
    "  .select('UnitPrice', 'isExpensive').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-bowling",
   "metadata": {},
   "source": [
    "- 위와 같이 필터조건을 표현식으로 작성해도 되지만 **칼럼명을 사용**해서도 작성할 수 있음\n",
    "    * 동작방식은 동일하기 때문에 표현식과 칼럼명 방법 사이의 속도 차이 존재 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "little-lottery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|   Description|isExpensive|\n",
      "+--------------+-----------+\n",
      "|DOTCOM POSTAGE|       true|\n",
      "|DOTCOM POSTAGE|       true|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.withColumn('isExpensive', expr('NOT UnitPrice <= 250'))\\\n",
    "  .where('isExpensive')\\\n",
    "  .select('Description', 'isExpensive').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-quarter",
   "metadata": {},
   "source": [
    "- 불리언 표현식을 만들 때 ``null`` 데이터는 어떻게 다루어야 할까?\n",
    "    * 다음과 같은 코드 수행\n",
    "- ``join``방법 중 ``left-anti join``이란 왼쪽 테이블에는 있으면서 오른쪽 테이블에는 없는 row들만 남김"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "robust-insured",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col('Description').eqNullSafe('Quantity')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-conditioning",
   "metadata": {},
   "source": [
    "- ``eqNullSafe(df.column)`` 메소드는 결측치를 없는 데이터로 간주함!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-idaho",
   "metadata": {},
   "source": [
    "## 수치형 데이터 타입 다루기\n",
    "- ``pow``를 활용해 거듭제곱 계산 가능(=Native Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "designing-survivor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|             489.0|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, pow\n",
    "\n",
    "fabricatedQuantity = pow(col('Quantity')*col('UnitPrice'), 2) + 5 # Column으로 반환됨\n",
    "df.select(expr('CustomerId'), fabricatedQuantity.alias('realQuantity')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fuzzy-eugene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|             489.0|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL 구문으로도 가능\n",
    "df.selectExpr(\"CustomerId\",\n",
    "             \"(POWER((Quantity * UnitPrice), 2) + 5) as realQuantity\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-functionality",
   "metadata": {},
   "source": [
    "- 반올림 메소드: ``round()``\n",
    "- 내림 메소드: ``bround()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tested-palestine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, round, bround\n",
    "\n",
    "# lit으로 스파크 자체 데이터 타입으로 변경\n",
    "df.select(round(lit(2.5)), bround(lit(2.5))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-squad",
   "metadata": {},
   "source": [
    "- 두 수치형 변수간의 ``Pearson Correlation`` 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "crude-peoples",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04112314436835551"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "df.stat.corr('Quantity', 'UnitPrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "linear-biotechnology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(corr('Quantity', 'UnitPrice')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-insider",
   "metadata": {},
   "source": [
    "- 하나 이상의 수치형 칼럼에 대한 요약 통계 계산 방법\n",
    "    * ``describe()`` 메서드 사용. 단, 통계 스키마가 변경될 수도 있으니 콘솔용에서 확인 가능\n",
    "    * 정확한 통계수치를 보기 위해서는 통계 요약값 계산 메소드를 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "backed-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|        InvoiceDate|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|               3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128|               null| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|               null|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|2010-12-01 08:26:00|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|2010-12-01 17:35:00|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "occupational-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, max, min, mean, stddev_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-ownership",
   "metadata": {},
   "source": [
    "- StatFunctions 패키지 활용해서도 가능. ``stat``속성으로 접근해서 사용가능한 DataFrame 메서드\n",
    "    * ex) ``approxQuantile(column, probability, relativeError)``\n",
    "        - ``probability``: 백분위(list/tuple)\n",
    "        - ``relativeError``: 목표값과 얼마만큼의 에러차이를 허용할 것인지(만약 0으로 설정시 목표치와 매우 근사하지만 연산 비용이 많이듬(expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "angry-workshop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.approxQuantile('UnitPrice', [0.5], 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-broadway",
   "metadata": {},
   "source": [
    "- StatFunctions는 ``crosstab``(교차표) 기능도 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "opened-camping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21080|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
      "|             22219|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21908|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22818|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|           15056BL|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             72817|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22545|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22988|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
      "|             22274|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             20750|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|            82616C|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21703|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22899|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
      "|             22379|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22422|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22769|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22585|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.crosstab('StockCode', 'Quantity').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "numeric-oklahoma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "| StockCode_freqItems|  Quantity_freqItems|\n",
      "+--------------------+--------------------+\n",
      "|[90214E, 20728, 2...|[200, 128, 23, 32...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.freqItems(['StockCode', 'Quantity']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-psychology",
   "metadata": {},
   "source": [
    "- StatFunctions의 ``monotonically_increasing_id()``: 모든 로우에 unique한 ID값을 만들어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "atlantic-outreach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "|                            2|\n",
      "|                            3|\n",
      "|                            4|\n",
      "+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df.select(monotonically_increasing_id()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-tamil",
   "metadata": {},
   "source": [
    "- 이외 StatFunctions에는 다양한 고급 알고리즘 메소드도 제공을 함. 필요 시 공식문서 참조하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-tenant",
   "metadata": {},
   "source": [
    "## 문자열 데이터 타입 다루기\n",
    "\n",
    "- ``initcap``: 주어진 문자열에서 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경\n",
    "- ``upper``: 모든 문자를 대문자로\n",
    "- ``lower``: 모든 문자를 소문자로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "protective-leadership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|initcap(Description)               |\n",
      "+-----------------------------------+\n",
      "|White Hanging Heart T-light Holder |\n",
      "|White Metal Lantern                |\n",
      "|Cream Cupid Hearts Coat Hanger     |\n",
      "|Knitted Union Flag Hot Water Bottle|\n",
      "|Red Woolly Hottie White Heart.     |\n",
      "+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "\n",
    "df.select(initcap(col('Description'))).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "through-customer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------+-----------------------------------+\n",
      "|Description                        |lower(Description)                 |upper(Description)                 |\n",
      "+-----------------------------------+-----------------------------------+-----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |white hanging heart t-light holder |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|WHITE METAL LANTERN                |white metal lantern                |WHITE METAL LANTERN                |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |cream cupid hearts coat hanger     |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|knitted union flag hot water bottle|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |red woolly hottie white heart.     |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------------------------------+-----------------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper, lower\n",
    "\n",
    "df.select(col('Description'),\n",
    "         lower(col('Description')),\n",
    "         upper(col('Description'))).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-scientist",
   "metadata": {},
   "source": [
    "- 공백 제거하거나 추가하는 작업도 가능\n",
    "    * ``ltrim, rtrim, lpad, rpad, trim``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "commercial-chart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+------------------+------------------+\n",
      "|ltrim(  HELLO  )|rtrim(  HELLO  )|lpad(HELLO, 15, #)|rpad(HELLO, 15, $)|\n",
      "+----------------+----------------+------------------+------------------+\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "|         HELLO  |           HELLO|   ##########HELLO|   HELLO$$$$$$$$$$|\n",
      "+----------------+----------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, lpad, rpad, trim\n",
    "\n",
    "hello_col = lit('  HELLO  ')\n",
    "df.select(ltrim(hello_col),\n",
    "         rtrim(hello_col),\n",
    "         lpad(lit('HELLO'), 15, '#'),\n",
    "         rpad(lit('HELLO'), 15, '$')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-series",
   "metadata": {},
   "source": [
    "### 문자열 - 정규표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-death",
   "metadata": {},
   "source": [
    "- 스파크는 자바 정규 표현식이 가진 능력을 활용\n",
    "- ``regexpr_extract``, ``regexpr_replace`` 메소드를 사용\n",
    "    * ex) ``regexpr_replace(column, expression, replacement)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "leading-snapshot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------+\n",
      "|color_clean                        |Description                        |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "|COLOR HANGING HEART T-LIGHT HOLDER |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|COLOR METAL LANTERN                |WHITE METAL LANTERN                |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|COLOR WOOLLY HOTTIE COLOR HEART.   |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "regex_string = 'BLACK|WHITE|RED|GREEN|BLUE' # | = or을 의미하는 듯!\n",
    "df.select(regexp_replace(col('Description'), regex_string, 'COLOR').alias('color_clean'),\n",
    "         col('Description')).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-southeast",
   "metadata": {},
   "source": [
    "- 이번엔 정규표현식이 아닌 ``translate(column, 'string', 'replacement')``를 사용\n",
    "    * 이 때 string 하나 자체를 replacement 단어로 교체하는 것이 아닌 한 문자열씨 매핑되서 치환됨(하단의 예시를 참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "challenging-hazard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------+\n",
      "|translate(Description, LEET, 1337) |Description                        |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "|WHI73 HANGING H3AR7 7-1IGH7 HO1D3R |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|WHI73 M37A1 1AN73RN                |WHITE METAL LANTERN                |\n",
      "|CR3AM CUPID H3AR7S COA7 HANG3R     |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|KNI773D UNION F1AG HO7 WA73R BO7713|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|R3D WOO11Y HO77I3 WHI73 H3AR7.     |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "\n",
    "df.select(translate(col('Description'), 'LEET', '1337'), # L -> 1, E -> 3, T -> 7로 치환됨\n",
    "         col('Description')).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-lloyd",
   "metadata": {},
   "source": [
    "- ``regex_extract(column, expression, 추출할 index)``를 사용해 특정 문자열을 추출 가능\n",
    "- 만약 해당 문자열이 없으면 빈 문자열을 반환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "valid-external",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------------+\n",
      "|extract_str|Description                        |\n",
      "+-----------+-----------------------------------+\n",
      "|WHITE      |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|WHITE      |WHITE METAL LANTERN                |\n",
      "|           |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|           |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|RED        |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "extract_str = '(BLACK|WHITE|RED|GREEN|BLUE)'\n",
    "df.select(regexp_extract(col('Description'), extract_str, 1).alias('extract_str'),\n",
    "         col('Description')).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-malta",
   "metadata": {},
   "source": [
    "- 특정 문자열의 존재여부를 파악하기 위해서 ``contains``메서드를 사용할 수 있지만 Pyspark에서는 ``instr``로 대체사용\n",
    "- 해당 메서드를 적용하면 불리언 타입으로 반환하기 때문에 row 필터링 작업이 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "noticed-newport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "\n",
    "containsBlack = instr(col('Description'), 'BLACK') >= 1  # BLACK이 있는 인덱스가 1이상부터면 존재하는 것이나 마찬가지!(참고로 스파크는 인덱스가 1부터 시작!)\n",
    "containsWhite = instr(col('Description'), 'WHITE') >= 1\n",
    "\n",
    "df.withColumn('hasSimpleColor', (containsBlack | containsWhite))\\\n",
    "  .where('hasSimpleColor')\\\n",
    "  .select('Description').show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-stamp",
   "metadata": {},
   "source": [
    "- 만약 필터링하고 싶은 문자열의 대상들이 여러개 또는 동적으로 변할때 ``locate`` 메소드를 활용해 다음과 같이 구현할 수 있음\n",
    "    * ``locate(substr, column)``: 문자열의 위치 인덱스(1부터 시작)를 정수로 반환\n",
    "- ``cast``: 특정 타입으로 변환(Tensorflow의 ``cast``와 역할 비슷)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "numeric-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, locate\n",
    "\n",
    "simpleColors = ['black', 'white', 'red', 'green', 'blue']\n",
    "\n",
    "def color_locator(column, color_string):\n",
    "    return locate(color_string.upper(), column)\\\n",
    "           .cast(\"boolean\")\\\n",
    "           .alias('is_' + color_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "breeding-equity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'CAST(locate(BLACK, Description, 1) AS BOOLEAN) AS `is_black`'>,\n",
       " Column<b'CAST(locate(WHITE, Description, 1) AS BOOLEAN) AS `is_white`'>,\n",
       " Column<b'CAST(locate(RED, Description, 1) AS BOOLEAN) AS `is_red`'>,\n",
       " Column<b'CAST(locate(GREEN, Description, 1) AS BOOLEAN) AS `is_green`'>,\n",
       " Column<b'CAST(locate(BLUE, Description, 1) AS BOOLEAN) AS `is_blue`'>,\n",
       " Column<b'unresolvedstar()'>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedColumns = [color_locator(df['Description'], c) for c in simpleColors]\n",
    "selectedColumns.append(expr('*')) # 이를 추가안하고 select 구문에 넣으면 Boolean 타입으로 들어감\n",
    "selectedColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "inside-maker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "|HAND WARMER RED POLKA DOT         |\n",
      "|RED COAT RACK PARIS FASHION       |\n",
      "+----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# white or red가 True인 행들만\n",
    "# list unpacking(*)\n",
    "df.select(*selectedColumns)\\\n",
    "  .where(expr('is_white OR is_red'))\\\n",
    "  .select('Description').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-rescue",
   "metadata": {},
   "source": [
    "## 날짜와 타임스탬프 데이터 타입 다루기\n",
    "- 스파크는 두 가지 종류의 시간 관련 정보만 집중적으로 관리\n",
    "    * ``date``: 달력 형태의 날짜\n",
    "    * ``timestamp``: 날짜와 시간 정보를 모두 가지는 정보\n",
    "- 보통 스파크는 ``inferSchema`` 옵션이 active할 때는 날짜와 타임스탬프 데이터 타입을 최대한 정확하게 식별하려 함\n",
    "- 스파크의 ``TimestampType``클래스는 초(second) 단위까지만 정밀도를 제공. 만약 밀리세컨드, 마이크로세컨드 단위를 다루려면 해당 타임스탬프 데이터를 ``Long`` 타입으로 변환해서 처리하는 우회 정책이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "elegant-manner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = spark.range(10)\\\n",
    "              .withColumn('today', current_date())\\\n",
    "              .withColumn('now', current_timestamp())\n",
    "dateDF.createOrReplaceTempView('dataTable')\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-minneapolis",
   "metadata": {},
   "source": [
    "- 금일 기준으로 전후 5일 날짜를 구하기\n",
    "    * ``date_add(column, num of days)``, ``date_sub`` 메소드 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "opposite-geology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_add(today, 5)|date_sub(today, 5)|\n",
      "+------------------+------------------+\n",
      "|2021-09-01        |2021-08-22        |\n",
      "|2021-09-01        |2021-08-22        |\n",
      "|2021-09-01        |2021-08-22        |\n",
      "|2021-09-01        |2021-08-22        |\n",
      "|2021-09-01        |2021-08-22        |\n",
      "+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "\n",
    "dateDF.select(date_add(col('today'), 5), date_sub(col('today'), 5)).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-serial",
   "metadata": {},
   "source": [
    "- 두 날짜 사이의 일 수를 반환하는 ``datediff``\n",
    "- 두 날짜 사이의 개월 수를 반환하는 ``months_between``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "accessible-merchandise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(today, week_ago)|\n",
      "+-------------------------+\n",
      "|7                        |\n",
      "|7                        |\n",
      "|7                        |\n",
      "|7                        |\n",
      "|7                        |\n",
      "+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, months_between\n",
    "\n",
    "dateDF.withColumn('week_ago', date_sub(col('today'), 7))\\\n",
    "      .select(datediff(col('today'), col('week_ago'))).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "coral-construction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -33.29032258|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date # date 데이터로 만드는 메소드 -> format 지정시 자바의 SimpleDateFormat 클래스가 지원하는 포맷을 사용!\n",
    "\n",
    "dateDF.select(to_date(lit('2016-01-01')).alias('start'),\n",
    "             to_date(lit('2018-10-10')).alias('end'))\\\n",
    "      .select(months_between(col('start'), col('end'))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-wilderness",
   "metadata": {},
   "source": [
    "- 데이터 포맷 지정을 사용해서 ``월-일``이 바뀐 날짜를 디버깅하는 방법\n",
    "    * ``to_date``는 데이터 포맷 지정이 옵션\n",
    "    * ``to_timestamp``는 데이터 포맷 지정이 필수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aggressive-rubber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 날짜 파싱 불가하면 Null을 반환. 원하는 형태는 2017년 11월 12일!\n",
    "dateDF.select(to_date(lit('2016-20-12')), to_date(lit('2017-12-11'))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "conscious-spectrum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     dateA|     dateB|\n",
      "+----------+----------+\n",
      "|2016-12-20|2017-11-12|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 그런데 12월 11일로 나옴 -> 데이터 포맷을 지정하자!\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "dateFormat = 'yyyy-dd-MM'\n",
    "clenaDateDF = spark.range(1)\\\n",
    "                   .withColumn('dateA', to_date(lit('2016-20-12'), dateFormat))\\\n",
    "                   .withColumn('dateB', to_date(lit('2017-12-11'), dateFormat))\\\n",
    "                   .select('dateA', 'dateB').show()\n",
    "# cleanDateDF = spark.range(1)\\\n",
    "#                    .select(to_date(lit('2016-20-12'), dateFormat).ailas('dateA'),\n",
    "#                            to_date(lit('2017-12-11'), dateFormat).alias('dateB'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "golden-buffalo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|              dateA|     dateB|\n",
      "+-------------------+----------+\n",
      "|2016-12-20 00:00:00|2017-11-12|\n",
      "+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to_timestamp\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "clenaDateDF = spark.range(1)\\\n",
    "                   .withColumn('dateA', to_timestamp(lit('2016-20-12'), dateFormat))\\\n",
    "                   .withColumn('dateB', to_date(lit('2017-12-11'), dateFormat))\\\n",
    "                   .select('dateA', 'dateB').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-northern",
   "metadata": {},
   "source": [
    "- 날짜 비교도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "stunning-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDateDF = spark.range(1)\\\n",
    "     .select(to_date(lit('2017-20-12'), dateFormat).alias('date'),\n",
    "            to_date(lit('2016-11-12'), dateFormat).alias('date2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "elder-privilege",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-12-20|2016-12-11|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col('date2') < lit('2020-01-01')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-phrase",
   "metadata": {},
   "source": [
    "- ``lit``사용 대신 스파크가 리터럴로 인식하는 문자열을 지정해 날짜를 비교할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "incorporated-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-12-20|2016-12-11|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col('date2') < \"2020-01-01\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-hartford",
   "metadata": {},
   "source": [
    "## ``null`` 값  다루기\n",
    "- 스파크에서는 ``null``값을 사용해야 최적화를 수행할 수 있음\n",
    "- 참고로 스키마의 모든 칼럼이 ``null``값을 허용하지 않다 하더라도 칼럼에 ``null``값을 넣을 수 있음. 그래서 만약 ``null``값이 없어야 하는 칼럼에 ``nul``값이 존재한다면 부정확한 결과나 디버깅하기 어려운 에러가 발생함.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-reference",
   "metadata": {},
   "source": [
    "### ``coalesce``\n",
    "- 해당 메소드 인자로 지정한 여러 칼럼 중 ``null``이 아닌 첫번째 값을 반환함\n",
    "- 만약 모든 칼럼이 ``null`` 값이 없다면 첫 번째 칼럼의 값을 반환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "comparable-metro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|coalesce(Description, CustomerId)|\n",
      "+---------------------------------+\n",
      "|             WHITE HANGING HEA...|\n",
      "|              WHITE METAL LANTERN|\n",
      "|             CREAM CUPID HEART...|\n",
      "|             KNITTED UNION FLA...|\n",
      "|             RED WOOLLY HOTTIE...|\n",
      "|             SET 7 BABUSHKA NE...|\n",
      "|             GLASS STAR FROSTE...|\n",
      "|             HAND WARMER UNION...|\n",
      "|             HAND WARMER RED P...|\n",
      "|             ASSORTED COLOUR B...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             FELTCRAFT PRINCES...|\n",
      "|             IVORY KNITTED MUG...|\n",
      "|             BOX OF 6 ASSORTED...|\n",
      "|             BOX OF VINTAGE JI...|\n",
      "|             BOX OF VINTAGE AL...|\n",
      "|             HOME BUILDING BLO...|\n",
      "|             LOVE BUILDING BLO...|\n",
      "|             RECIPE BOX WITH M...|\n",
      "+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "df.select(coalesce(col('Description'), col('CustomerId'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-effects",
   "metadata": {},
   "source": [
    "- Description 칼럼에 대한 값을 반환하는 거 보니 모든 칼럼에 대해 ``null``값이 존재하지 않다는 것을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-noise",
   "metadata": {},
   "source": [
    "### ``ifnull, nullif, nvl, nvl2``\n",
    "- ``ifnull(null, 'return_value')`` => 첫 번째 값이 null이면 두번째, null이 아니라면 첫 번째 값을 반환\n",
    "- ``nullif('value', 'value')`` => 두 값이 같으면 Null을 반환\n",
    "- ``nvl(null, 'return_value')`` => 첫 번째 값이 null이면 두 번재값을 반환... 즉, ``ifnull``이랑 동일\n",
    "- ``nvl2(not_null, 'return_value', 'else_value')`` => 첫 번째 값이 null이 아니면 두번째값을 반환하고, Null이면 세번째 값을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-property",
   "metadata": {},
   "source": [
    "### ``drop``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "final-english",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop()\n",
    "df.na.drop('any')  # 하나의 na라도 있으면 제거\n",
    "df.na.drop('all')  # 모든 칼럼에 대해 na값이 있는 로우들만 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "returning-tours",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop인자에 array 형태로 칼럼명을 전달해서 결측치 제거할 수 있음\n",
    "df.na.drop('all', subset=['StockCode', 'InvoiceNo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-concern",
   "metadata": {},
   "source": [
    "### ``fill``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "advanced-territory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.fill(\"All Null values become this string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "corresponding-repair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.fill('all', subset=['StockCode', 'InvoiceNo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bored-creator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python의 딕셔너리를 이용해서도 가능\n",
    "fill_dict = {'StockCode': 5, 'Description': 'Hello world'}\n",
    "df.na.fill(fill_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-nitrogen",
   "metadata": {},
   "source": [
    "### ``replace``\n",
    "- 단, 변경하고자하는 값과 이전 값의 데이터 타입이 동일해야 함\n",
    "- ``df.na.replace([이전값], [바꿀값], \"column_name\")``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "moving-playlist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.replace([\"\"], [\"Unknown\"], \"Description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-singapore",
   "metadata": {},
   "source": [
    "- ``asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last`` 메소드를 통해 결측치를 포함하는 로우들을 정렬 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-blade",
   "metadata": {},
   "source": [
    "## 복합 데이터 타입 다루기\n",
    "### 구조체\n",
    "- 구조체: DataFrame 내부의 DataFrame. 쿼리문에서 다수의 칼럼을 괄호로 묶어 구조체를 만들 수 있음\n",
    "    * 일반적인 DataFrame 조회하는 것처럼 사용가능 단, 문법에 ``.``을 사용하거나 ``getField`` 메서드를 사용\n",
    "    * 또, ``*``를 사용해 모든 값을 조회할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "labeled-dakota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|complex                                      |\n",
      "+---------------------------------------------+\n",
      "|[WHITE HANGING HEART T-LIGHT HOLDER, 536365] |\n",
      "|[WHITE METAL LANTERN, 536365]                |\n",
      "|[CREAM CUPID HEARTS COAT HANGER, 536365]     |\n",
      "|[KNITTED UNION FLAG HOT WATER BOTTLE, 536365]|\n",
      "|[RED WOOLLY HOTTIE WHITE HEART., 536365]     |\n",
      "+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "complexDF.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fabulous-dubai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.select(\"complex.Description\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dangerous-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|complex.Description               |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.select(col(\"complex\").getField('Description')).show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "russian-cameroon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+---------+\n",
      "|Description                        |InvoiceNo|\n",
      "+-----------------------------------+---------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |536365   |\n",
      "|WHITE METAL LANTERN                |536365   |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |536365   |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365   |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |536365   |\n",
      "+-----------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.select(\"complex.*\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-dubai",
   "metadata": {},
   "source": [
    "### 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "manual-outside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "necessary-acting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|split(Description,  , -1)               |\n",
      "+----------------------------------------+\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|\n",
      "|[WHITE, METAL, LANTERN]                 |\n",
      "+----------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 우선 split 메소드를 사용해 배열로 변환\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df.select(split(col(\"Description\"), \" \")).show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "earned-committee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[1]|\n",
      "+------------+\n",
      "|HANGING     |\n",
      "|METAL       |\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 파이썬 인덱싱 문법과 유사하게 split한 배열의 특정 요소에 접근이 가능\n",
    "# 특이하게 Expr에 문자열 형태로 인덱싱 []을 입력해주어도 알아듣는다!\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n",
    "  .selectExpr(\"array_col[1]\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "hired-daily",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|size(split(Description,  , -1))|\n",
      "+-------------------------------+\n",
      "|5                              |\n",
      "|3                              |\n",
      "+-------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 배열의 길이 알아내기\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "emotional-toilet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|true                                            |\n",
      "|true                                            |\n",
      "|false                                           |\n",
      "+------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 배열에 특정 값이 존재하는지 확인 가능\n",
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), 'WHITE')).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-cause",
   "metadata": {},
   "source": [
    "- ``array_contains``는 단순히 Boolean 타입으로만 반환함. 이를 로우로 변환시켜주기 위해서는 ``explode``를 사용!\n",
    "    * ``explode``는 입력된 칼럼의 배열값에 포함된 모든 값을 하나의 row로 모두 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "elder-visitor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+---------+------------------------------------------+--------+\n",
      "|Description                        |InvoiceNo|splitted                                  |exploded|\n",
      "+-----------------------------------+---------+------------------------------------------+--------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |536365   |[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |WHITE   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |536365   |[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |HANGING |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |536365   |[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |HEART   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |536365   |[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |T-LIGHT |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |536365   |[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |HOLDER  |\n",
      "|WHITE METAL LANTERN                |536365   |[WHITE, METAL, LANTERN]                   |WHITE   |\n",
      "|WHITE METAL LANTERN                |536365   |[WHITE, METAL, LANTERN]                   |METAL   |\n",
      "|WHITE METAL LANTERN                |536365   |[WHITE, METAL, LANTERN]                   |LANTERN |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |536365   |[CREAM, CUPID, HEARTS, COAT, HANGER]      |CREAM   |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |536365   |[CREAM, CUPID, HEARTS, COAT, HANGER]      |CUPID   |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |536365   |[CREAM, CUPID, HEARTS, COAT, HANGER]      |HEARTS  |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |536365   |[CREAM, CUPID, HEARTS, COAT, HANGER]      |COAT    |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |536365   |[CREAM, CUPID, HEARTS, COAT, HANGER]      |HANGER  |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365   |[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|KNITTED |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365   |[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|UNION   |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365   |[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|FLAG    |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365   |[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|HOT     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365   |[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|WATER   |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365   |[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|BOTTLE  |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |536365   |[RED, WOOLLY, HOTTIE, WHITE, HEART.]      |RED     |\n",
      "+-----------------------------------+---------+------------------------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "df.withColumn('splitted', split(col('Description'), \" \"))\\\n",
    "  .withColumn('exploded', explode(col('splitted')))\\\n",
    "  .select(\"Description\", \"InvoiceNo\", \"splitted\", \"exploded\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-motion",
   "metadata": {},
   "source": [
    "### ``map``\n",
    "- 맵함수와 칼럼의 키-값 쌍을 이용해 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bizarre-resource",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|complex_map                                   |\n",
      "+----------------------------------------------+\n",
      "|[WHITE HANGING HEART T-LIGHT HOLDER -> 536365]|\n",
      "|[WHITE METAL LANTERN -> 536365]               |\n",
      "+----------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "  .show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-fruit",
   "metadata": {},
   "source": [
    "- 위 그림에서 [key -> value] 가 됨\n",
    "- 그래서 key를 이용해 그 key에 매핑된 value 조회 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "passing-financing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|null                            |\n",
      "|536365                          |\n",
      "+--------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "  .selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-vatican",
   "metadata": {},
   "source": [
    "- 해당 키가 없다면 ``null``을 반환함! 즉, 모든 row에 대해 각각 다 비교한 다음 키가 없다는 의미로 ``null``을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-freeware",
   "metadata": {},
   "source": [
    "- 위와 같이 ``map``타입은 ``explode``를 사용해 key-value를 칼럼별로 따로 떼어놓게 변환 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "north-physics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------+\n",
      "|key                                |value |\n",
      "+-----------------------------------+------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |536365|\n",
      "|WHITE METAL LANTERN                |536365|\n",
      "|CREAM CUPID HEARTS COAT HANGER     |536365|\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |536365|\n",
      "|SET 7 BABUSHKA NESTING BOXES       |536365|\n",
      "|GLASS STAR FROSTED T-LIGHT HOLDER  |536365|\n",
      "|HAND WARMER UNION JACK             |536366|\n",
      "|HAND WARMER RED POLKA DOT          |536366|\n",
      "|ASSORTED COLOUR BIRD ORNAMENT      |536367|\n",
      "|POPPY'S PLAYHOUSE BEDROOM          |536367|\n",
      "|POPPY'S PLAYHOUSE KITCHEN          |536367|\n",
      "|FELTCRAFT PRINCESS CHARLOTTE DOLL  |536367|\n",
      "|IVORY KNITTED MUG COSY             |536367|\n",
      "|BOX OF 6 ASSORTED COLOUR TEASPOONS |536367|\n",
      "|BOX OF VINTAGE JIGSAW BLOCKS       |536367|\n",
      "|BOX OF VINTAGE ALPHABET BLOCKS     |536367|\n",
      "|HOME BUILDING BLOCK WORD           |536367|\n",
      "|LOVE BUILDING BLOCK WORD           |536367|\n",
      "|RECIPE BOX WITH METAL HEART        |536367|\n",
      "+-----------------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "  .selectExpr(\"explode(complex_map)\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-endorsement",
   "metadata": {},
   "source": [
    "### JSON 다루기\n",
    "- JSON 조작, JSON 파싱, JSON 객체로 생성까지 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "proved-illinois",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[{\"myJSONKey\": {\"myJSONValue\": [1,2,3,]}} AS jsonString: string]\n",
      "+------------------------------------------------------+\n",
      "|{\"myJSONKey\": {\"myJSONValue\": [1,2,3,]}} AS jsonString|\n",
      "+------------------------------------------------------+\n",
      "|                                  {\"myJSONKey\": {\"m...|\n",
      "+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.JSON 객체 생성\n",
    "jsonDF = spark.range(1) \\\n",
    "              .selectExpr(\"\"\"\n",
    "              '{\"myJSONKey\": {\"myJSONValue\": [1,2,3,]}} AS jsonString'\n",
    "              \"\"\")\n",
    "print(jsonDF)\n",
    "jsonDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "rising-ukraine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   to_json(myStruct)|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JSON 문자열로 변경하기\n",
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "json_test = df.selectExpr(\"(InvoiceNo, Description) as myStruct\") \\\n",
    "              .select(to_json(col(\"myStruct\")))\n",
    "json_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "independent-despite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|  from_json(newJson)|             newJson|\n",
      "+--------------------+--------------------+\n",
      "|[536365, WHITE HA...|{\"InvoiceNo\":\"536...|\n",
      "|[536365, WHITE ME...|{\"InvoiceNo\":\"536...|\n",
      "|[536365, CREAM CU...|{\"InvoiceNo\":\"536...|\n",
      "|[536365, KNITTED ...|{\"InvoiceNo\":\"536...|\n",
      "|[536365, RED WOOL...|{\"InvoiceNo\":\"536...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JSON 객체에서 일반 객체로 변경하기 -> 대신 스키마 꼭 지정해주어야 함!\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "parseSchema = StructType([\n",
    "    StructField('InvoiceNo', StringType(), True),\n",
    "    StructField('Description', StringType(), True)\n",
    "])\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\") \\\n",
    "  .select(to_json(col(\"myStruct\")).alias(\"newJson\")) \\\n",
    "  .select(from_json(col(\"newJson\"), parseSchema), col('newJson')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-winter",
   "metadata": {},
   "source": [
    "### 사용자 정의 함수(UDF)\n",
    "- 파이썬이나 외부 라이브러리를 활용해 사용자가 원하는 형태로 트랜스포메이션이 가능\n",
    "- 하나 이상의 칼럼을 입력받고 반환 가능\n",
    "- 레코드별로 데이터를 처리함\n",
    "- SparkSession이나 Context에서도 UDF를 사용할 수 있도록 하기 위해 스파크에 만든 UDF를 등록하는 절차를 밟아야 함!\n",
    "- 함수 동작 과정\n",
    "    1. 스파크세션이 있는 드라이버에서 UDF를 직렬화 시키고 각 워커 노드에 전달\n",
    "    2. 스파크에서 파이썬 프로세스를 실행한 후 각 워커 노드에 데이터 전달\n",
    "    3. 각 워커 노드에서 함수를 실행한 후 결과값을 반환\n",
    "    - 특히, UDF를 직렬화 하는 과정에서 부하가 많이 발생\n",
    "    - 데이터가 파이썬으로 전달되면 더이상 스파크에서 워커 메모리를 관리할 수 없게 되어 JVM과 파이썬이 동일한 머신에서 메모리 경합을 벌이게 되고 리소스 분배에 제약이 생김. 그러므로 UDF는 스칼라, 자바로 작성하는 것을 권장한다고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "selected-ministry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExampleDf = spark.range(5).toDF(\"num\")\n",
    "udfExampleDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "operating-academy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.power3(double_value)>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UDF 정의\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "\n",
    "# UDF를 스파크에 등록\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "power3udf = udf(power3)\n",
    "power3udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-deputy",
   "metadata": {},
   "source": [
    "- 스파크에 등록한 UDF를 spark_udf 라고 할때, ``spark_udf(col('column_name'))`` 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "acquired-creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "|         27|\n",
      "|         64|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF를 스파크 데이터프레임에 적용\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "udfExampleDf.select(power3udf(col('num'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-scott",
   "metadata": {},
   "source": [
    "- 단지 스파크에 등록한 UDF를 문자열표현식에는 사용할 수 없지만 스파크 SQL에 등록하면 모든 프로그래밍 언어와 SQL에서 사용자 정의함수를 사용할 수 있음!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "vanilla-equality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|power3py(num)|\n",
      "+-------------+\n",
      "|            0|\n",
      "|            1|\n",
      "|            8|\n",
      "|           27|\n",
      "|           64|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스파크 SQL로 UDF를 등록!\n",
    "spark.udf.register('power3py', power3)\n",
    "\n",
    "udfExampleDf.selectExpr(\"power3py(num)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-referral",
   "metadata": {},
   "source": [
    "- 그런데, 스파크 SQL에 UDF를 등록할 때, 반환되는 데이터 타입을 지정하는 것을 권장. 만약 지정하지 않은 상태에서 반환되는 실제 데이터 타입과 일치하지 않는 타입이면 null을 반환.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "later-sight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|power3py_2(num)|\n",
      "+---------------+\n",
      "|           null|\n",
      "|           null|\n",
      "|           null|\n",
      "|           null|\n",
      "|           null|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType # 스파크의 DoubleType = 파이썬의 Float\n",
    "\n",
    "spark.udf.register('power3py_2', power3, DoubleType())\n",
    "\n",
    "udfExampleDf.selectExpr(\"power3py_2(num)\").show()  # 반환되는 값이 Integer인데, Double을 지정했으니 Null이 반환됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "employed-excellence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|power3py_3(num)|\n",
      "+---------------+\n",
      "|              0|\n",
      "|              1|\n",
      "|              8|\n",
      "|             27|\n",
      "|             64|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register('power3py_3', power3, IntegerType())\n",
    "\n",
    "udfExampleDf.selectExpr(\"power3py_3(num)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-anniversary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
