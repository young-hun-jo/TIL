{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "altered-worst",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#구조적-스트리밍(Structured-Streaming)\" data-toc-modified-id=\"구조적-스트리밍(Structured-Streaming)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>구조적 스트리밍(Structured Streaming)</a></span></li><li><span><a href=\"#머신러닝과-고급분석\" data-toc-modified-id=\"머신러닝과-고급분석-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>머신러닝과 고급분석</a></span><ul class=\"toc-item\"><li><span><a href=\"#데이터-전처리\" data-toc-modified-id=\"데이터-전처리-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>데이터 전처리</a></span></li><li><span><a href=\"#데이터-분할\" data-toc-modified-id=\"데이터-분할-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>데이터 분할</a></span></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Feature Engineering</a></span></li><li><span><a href=\"#Vector-화-시키기\" data-toc-modified-id=\"Vector-화-시키기-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Vector 화 시키기</a></span></li><li><span><a href=\"#Pipeline-화-시키기\" data-toc-modified-id=\"Pipeline-화-시키기-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Pipeline 화 시키기</a></span></li><li><span><a href=\"#모델-학습\" data-toc-modified-id=\"모델-학습-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>모델 학습</a></span></li><li><span><a href=\"#Cost(비용)-평가\" data-toc-modified-id=\"Cost(비용)-평가-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Cost(비용) 평가</a></span></li></ul></li><li><span><a href=\"#저수준-API인-RDD\" data-toc-modified-id=\"저수준-API인-RDD-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>저수준 API인 RDD</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-grave",
   "metadata": {},
   "source": [
    "# 구조적 스트리밍(Structured Streaming)\n",
    "\n",
    "- 스트림 처리용 고수준 API\n",
    "- 구조적 API(Dataset, DataFrame, SQL)로 개발된 배치 모드의 연산을 스트리밍 방식으로 실행 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "welcome-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = spark\\\n",
    "                  .read.format('csv')\\\n",
    "                  .option('header', 'true')\\\n",
    "                  .option('inferSchema', 'true')\\\n",
    "                  .load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/retail-data/by-day/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "editorial-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL 사용하기 위해 임시 테이블 등록\n",
    "staticDataFrame.createOrReplaceTempView('retail_data')\n",
    "\n",
    "# 로드한 csv 데이터들의 스키마를 객체로 할당\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "diverse-gambling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,StringType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,DoubleType,true),StructField(Country,StringType,true))) \n",
      " <class 'pyspark.sql.types.StructType'>\n"
     ]
    }
   ],
   "source": [
    "print(staticSchema, '\\n', type(staticSchema))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-subject",
   "metadata": {},
   "source": [
    "- 시계열 데이터를 다룰 때 타임스태프를 자주 다루기 떄문에 **윈도우 함수**를 자주 사용\n",
    "- *총 구매비용 칼럼을 추가하고 고객별로 가장 많이 소비한 날*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "metallic-harrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticDataFrame.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cleared-pickup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerID|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   17450.0|[2011-09-20 09:00...|          71601.44|\n",
      "|      null|[2011-11-14 09:00...|          55316.08|\n",
      "|      null|[2011-11-07 09:00...|          42939.17|\n",
      "|      null|[2011-03-29 09:00...| 33521.39999999998|\n",
      "|      null|[2011-12-08 09:00...|31975.590000000007|\n",
      "+----------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame 구문\n",
    "from pyspark.sql.functions import window, col, desc\n",
    "\n",
    "# 그룹핑 기준을 2개: 고객별, 날짜(1일) 별\n",
    "staticDataFrame\\\n",
    ".selectExpr(\"CustomerID\", \"(UnitPrice * Quantity) AS total_cost\", \"InvoiceDate\")\\\n",
    ".groupBy(col(\"CustomerID\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")\\\n",
    ".sort(desc(\"sum(total_cost)\"))\\\n",
    ".limit(5)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "transsexual-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle 파티션 수정하기\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-cattle",
   "metadata": {},
   "source": [
    "- 스트리밍 코드로 변환하기\n",
    "- ``maxFilesPerTrigger``로 한 번에 읽을 파일 개수 설정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "drawn-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame = spark\\\n",
    "                     .readStream\\\n",
    "                     .schema(staticSchema)\\\n",
    "                     .option('maxFilesPerTrigger', 1)\\\n",
    "                     .format('csv')\\\n",
    "                     .option('header', 'true')\\\n",
    "                     .load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/retail-data/by-day/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "round-effects",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스트리밍 유형 데이터프레임인지 확인\n",
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-subsection",
   "metadata": {},
   "source": [
    "- 스트리밍 코드의 ``Transformations``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "south-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고객 별로 하루 당 총 판매금액 계산하는 로직\n",
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    ".selectExpr(\"CustomerID\", \"(UnitPrice * Quantity) AS total_cost\", \"InvoiceDate\")\\\n",
    ".groupBy(col(\"CustomerID\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-pharmacology",
   "metadata": {},
   "source": [
    "- 코드실행위해 스트리밍 코드를 ``Action`` 하기\n",
    "- 단, 스트리밍 액션은 ``count``와 같은 정적 액션과는 다름\n",
    "- **트리거**가 실행된 다음 데이터를 갱신하게 될 인메모리 테이블에 데이터를 저장함\n",
    "- 여기서는 파일마다 **트리거**가 실행됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vanilla-delivery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fa9fa8fce50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Args:\n",
    "    - memory : 인메모리 테이블에 데이터를 저장\n",
    "    - customer_purchases : 인메모리에 저장될 테이블 이름\n",
    "    - complete : 코드 수행 결과 모든 것을 테이블에 저장\n",
    "\"\"\"\n",
    "purchaseByCustomerPerHour.writeStream\\\n",
    ".format(\"memory\")\\\n",
    ".queryName(\"customer_purchases\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "numerical-cameroon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------+\n",
      "|CustomerID|              window|sum(total_cost)|\n",
      "+----------+--------------------+---------------+\n",
      "|   13329.0|[2010-12-08 09:00...|          304.2|\n",
      "|   12797.0|[2010-12-09 09:00...|         254.03|\n",
      "|   16250.0|[2010-12-01 09:00...|         226.14|\n",
      "|   15660.0|[2010-12-09 09:00...|          209.3|\n",
      "|   17460.0|[2010-12-01 09:00...|           19.9|\n",
      "+----------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 위에서 인메모리에 저장한 테이블에 데이터가 어떻게 기록되었는지 확인\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM customer_purchases\n",
    "ORDER BY 'sum(total_cost)' DESC\"\"\")\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "defined-mumbai",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fa9fa8f0f50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# console에 출력시키기\n",
    "purchaseByCustomerPerHour.writeStream\\\n",
    ".format(\"console\")\\\n",
    ".queryName(\"customer_purchases_2\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-flood",
   "metadata": {},
   "source": [
    "# 머신러닝과 고급분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "junior-dubai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-alabama",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "- Timestamp 변수에서 수치형 변수로 파생변수 만들기\n",
    "- ``coalesce(num_of_partitions)``: RDD/DF의 파티션의 개수를 효율적인 방법으로 몇 개로 줄일 건지\n",
    "    * 이와 비슷하게 파티션 개수를 늘였다 줄였다 할 수 있는 ``repartition()``도 있음!\n",
    "    * 위 두 개의 메소드는 공통적으로 shuffle을 사용하는 Transformation이기 때문에 상당히 expensive한 operations들임. 그래서 이를 가능한 한 효율적으로 해주어야 함\n",
    "    * ``coalesce()``가 ``repartiton()`` 메소드를 operation 비용에서 좀 더 최적화시킴. 파티션 간의 데이터 shuffle를 보다 낮게 이동. 예를 들면 기존의 파티션 5개(1번~5번) 중 3개로 줄이면 기존의 파티션 중 2개의 파티션에 들어있는 데이터들을 다른 파티션(3개)로 분할하면서 데이터의 이동을 줄임\n",
    "    * 하지만 파티션을 **줄일**때만 사용 가능하다는 것\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "silver-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "\n",
    "# day of week(요일) 변수 새로 만들기 -> 'EEEE' -> Monday 문자열 다나옴, 'EEE' -> Mon까지만 나옴\n",
    "preppedDataFrame = staticDataFrame\\\n",
    ".na.fill(0)\\\n",
    ".withColumn('day_of_week', date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n",
    ".coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "widespread-blast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|day_of_week|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|     Monday|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preppedDataFrame.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-salvation",
   "metadata": {},
   "source": [
    "## 데이터 분할\n",
    "- 데이터셋 분할 트랜스포메이션 API: ``TrainValidationSplit()``, ``CrossValidator()``도 있음\n",
    "- 여기 예제에서는 ``where()``를 활용해 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "racial-effect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 245903\n",
      "Test: 296006\n"
     ]
    }
   ],
   "source": [
    "trainDataFrame = preppedDataFrame.where(\"InvoiceDate < '2011-07-01'\")\n",
    "testDataFrame = preppedDataFrame.where(\"InvoiceDate >= '2011-07-01'\")\n",
    "\n",
    "# count of each dataframe\n",
    "print('Train:', trainDataFrame.count())\n",
    "print('Test:', testDataFrame.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-therapy",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "- 범주형 변수를 수치형으로 1:1 변환. 마치 Label Encoder임. 여기서는 ``StringIndexer``을 사용\n",
    "- 하지만 만약 범주형 변수가 명목형 변수라고 한다면 서열 값이 반영되기 때문에 잘못된 방식임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "meaningful-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer()\\\n",
    ".setInputCol('day_of_week')\\\n",
    ".setOutputCol('day_of_week_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-twist",
   "metadata": {},
   "source": [
    "- 명목형 변수를 Boolean 수치형 변수로 바꾸기 위해 ``StringIndex`` 수치값을 ``OneHotEncoder`` 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proved-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\\\n",
    ".setInputCol('day_of_week_index')\\\n",
    ".setOutputCol('day_of_week_encoded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-humor",
   "metadata": {},
   "source": [
    "- 위 ``StringIndexer``, ``OneHotEncoder`` 결과는 모두 벡터 타입을 구성할 컬럼 중 하나로 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-heater",
   "metadata": {},
   "source": [
    "## Vector 화 시키기\n",
    "- Spark MLlib 머신러닝 알고리즘 입력으로 넣기 위해 변수들을 **벡터화**시켜야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "floral-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler()\\\n",
    ".setInputCols(['UnitPrice', 'Quantity', 'day_of_week_encoded'])\\\n",
    ".setOutputCol('features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-cylinder",
   "metadata": {},
   "source": [
    "## Pipeline 화 시키기\n",
    "- 요일(day of week)을 레이블 인코딩, 원-핫 인코딩 시키는 Transformation, 그리고 변수들을 벡터화시키는 벡터 어셈블러 Transformation을 파이프라인으로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "altered-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationsPipeline = Pipeline()\\\n",
    ".setStages([indexer, encoder, vectorAssembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "threaded-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 변환자(Transformer)를 적합(fit)시키기\n",
    "fittedPipeline = transformationsPipeline.fit(trainDataFrame)\n",
    "\n",
    "# 2. 파이프라인에 지정된 변환자들 사용해 데이터 변환\n",
    "transformedTraining = fittedPipeline.transform(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "defensive-thirty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string, day_of_week: string, day_of_week_index: double, day_of_week_encoded: vector, features: vector]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중간 변환된 데이터셋의 복사본을 메모리에 저장하는 캐싱코드 -> 이를 제거하고 학습 시 속도 차이가 매우 큼\n",
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-opera",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "controlling-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans().setK(20).setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "after-following",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소요 시간: 2.778 초\n"
     ]
    }
   ],
   "source": [
    "# Pipeline 할당(캐싱버전)\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "kmModel = kmeans.fit(transformedTraining)\n",
    "print(\"소요 시간:\", round(time.time() - start, 3),'초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "proud-incidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소요 시간: 5.428 초\n"
     ]
    }
   ],
   "source": [
    "# Pipeline 할당(노캐싱버전)\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "kmModel = kmeans.fit(transformedTraining)\n",
    "print(\"소요 시간:\", round(time.time() - start, 3),'초')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-trading",
   "metadata": {},
   "source": [
    "## Cost(비용) 평가\n",
    "- 현재 KMeans 알고리즘이기 때문에 Cost(비용) = '각 군집 중심점과의 제곱거리의 합'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "digital-shipping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터에 대한 실루엣 지표: 0.6842576726028763\n"
     ]
    }
   ],
   "source": [
    "# Train 데이터에 대한 Cost\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Train 데이터에 대해 예측\n",
    "train_pred = kmModel.transform(transformedTraining)\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "train_silhouette = evaluator.evaluate(train_pred)\n",
    "\n",
    "print(\"Train 데이터에 대한 실루엣 지표:\", train_silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "missing-engine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 데이터에 대한 실루엣 지표: 0.5427938390491535\n"
     ]
    }
   ],
   "source": [
    "# Test 데이터에 대한 Cost\n",
    "test_pred = kmModel.transform(transformedTest)\n",
    "\n",
    "test_silhouette = evaluator.evaluate(test_pred)\n",
    "\n",
    "print(\"Test 데이터에 대한 실루엣 지표:\", test_silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-mason",
   "metadata": {},
   "source": [
    "# 저수준 API인 RDD\n",
    "\n",
    "- 대부분은 ``Dataset, DataFrame, SQL``과 같은 구조적 API를 사용하는 것이 좋음\n",
    "- 그래도 RDD 이용하면 DataFrame 보다 더 세밀한 제어 가능\n",
    "- 드라이버 시스템의 메모리에 저장된 원시 데이터를 병렬처리(parallelize)하는 데 RDD를 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "convertible-logic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 간단하게 RDD로 만들고 DataFrame으로 변환하기\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rdd_df = spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()\n",
    "rdd_df.show() # 액션!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-theology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
